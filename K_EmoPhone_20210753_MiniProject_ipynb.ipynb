{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS565-DS522 IoT Data Science Mini Project for K-EmoPhone dataset\n",
        "*This material is a joint work of TAs from IC Lab at KAIST, including Panyu Zhang, Soowon Kang, and Woohyeok Choi. This work is licensed under CC BY-SA 4.0.*"
      ],
      "metadata": {
        "id": "cqjSsKeM3fhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instruction\n",
        "In this mini-project, we will build a model to predict users' self-reported stress using extracted features from K-EmoPhone dataset. This material mainly refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) conducting indepedent reproducibility experiments on K-EmoPhone dataset. In order to save time, we provide the extracted features from the raw data instead of starting from scratch. Besides, traditional machine learning model is used considering limited number of labels and multimodality issue in the in-the-wild K-EmoPhone dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "TiQ3qZRo52Wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guidance\n",
        "\n",
        "1. Before running the code, please first download the extracted features from the following [link](https://drive.google.com/file/d/1HcyFvzWEzO21osyP5E8VpVmHROX1ew7q/view?usp=sharing).\n",
        "\n",
        "2. Please change your runtime type to T4-GPU or other runtime types with GPU available since later we may use GPU for\n",
        "xgboost execution"
      ],
      "metadata": {
        "id": "Vbgf8BGx5MsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install latest version of xgboost > 2.0.0"
      ],
      "metadata": {
        "id": "YH4izzUnJ6hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQDlc8HW-Kt4",
        "outputId": "a3fc572f-e037-4468-d70d-3071b0e3805a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytz\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import cloudpickle\n",
        "from datetime import datetime\n",
        "from contextlib import contextmanager\n",
        "import warnings\n",
        "import time\n",
        "from typing import Optional\n",
        "from contextlib import contextmanager\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
        "\n",
        "RANDOM_STATE =42\n",
        "\n",
        "\n",
        "def log(msg: any):\n",
        "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))"
      ],
      "metadata": {
        "id": "XnKkV_aTVZ_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Preparation"
      ],
      "metadata": {
        "id": "0aF5NVMMUzBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Mount to Your Google Drive"
      ],
      "metadata": {
        "id": "oG5n57OnHIOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ9KU0Kq4WZs",
        "outputId": "408d9fb1-009a-499b-8875-84b40ce9dff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Load Extracted Features"
      ],
      "metadata": {
        "id": "Q9u_nJzcIKXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "'''Please specify your dataset path in your Google Drive'''\n",
        "PATH = '/content/drive/MyDrive/IoT_Data_Science/KEmoPhone/features_stress_fixed_K-EmoPhone.pkl'\n",
        "\n",
        "X, y, groups, t, datetimes = pickle.load(open(PATH, mode='rb'))"
      ],
      "metadata": {
        "id": "eg5JSHftIP_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e526e5-42cd-4275-fa59-92435c2f4290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-1989390981>:7: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  X, y, groups, t, datetimes = pickle.load(open(PATH, mode='rb'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "X is the extracted features and the feature extraction process refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) and the immediate past time window is set as 15 minutes. y is the array of labels while groups is the user ids.\n",
        "\n",
        "Please note that here y is binarized using theoretical threshold (if ESM stress > 0, binarize as 1, else 0, ESM label scale [-3, 3])"
      ],
      "metadata": {
        "id": "ffYa1CuFIx7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since features are already extracted, we do not need to work on preprocessing and feature extraction again."
      ],
      "metadata": {
        "id": "Tc0jkopjUl2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Feature Preparation\n"
      ],
      "metadata": {
        "id": "AGLIYf29UYES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There exist multiple types of features. Please try different combinations of features to see if there is any model performance improvement."
      ],
      "metadata": {
        "id": "xOgjC9HXo5cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#The following code is designed for reordering the data\n",
        "#################################################\n",
        "# Create a DataFrame with user_id and datetime\n",
        "\n",
        "df = pd.DataFrame({'user_id': groups, 'datetime': datetimes, 'label': y})\n",
        "\n",
        "# df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "\n",
        "# Sort the DataFrame by datetime\n",
        "df_merged = df_merged.sort_values(by=['user_id', 'datetime'])\n",
        "\n",
        "# Update groups and datetimes\n",
        "groups = df_merged['user_id'].to_numpy()\n",
        "datetimes = df_merged['datetime'].to_numpy()\n",
        "y = df_merged['label'].to_numpy()\n",
        "\n",
        "#X with all the features\n",
        "X_cleaned = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
        "\n",
        "\n",
        "#Divide the features into different categories\n",
        "feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]\n",
        "feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]\n",
        "feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]\n",
        "feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]\n",
        "\n",
        "feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
        "\n",
        "#################################################################################\n",
        "#Below are the available features\n",
        "#Divide the time window features into sensor/ESM self-report features\n",
        "feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]] #Current sensor features (value right before label)\n",
        "feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]] #Current ESM features (value right before label)\n",
        "feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]] #Immediate past sensor features (in past 15 minutes before label)\n",
        "feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  #Immediate past ESM features\n",
        "feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]] #Today epoch sensor features\n",
        "feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]] #Today epoch ESM features\n",
        "feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]] #Yesterday sensor features\n",
        "feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]] #Yesterday ESM features\n",
        "\n",
        "feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]\n",
        "feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]\n",
        "feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]\n",
        "################################################################################\n",
        "\n",
        "#Prepare the final feature set\n",
        "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
        "feat_final = pd.concat([feat_baseline],axis=1)\n",
        "\n",
        "################################################################################\n",
        "#X for the baseline originally given in the notebook\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]"
      ],
      "metadata": {
        "id": "PE57GSJPVOWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OzM5J1Fy1Jwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_groups = {\n",
        "    \"feat_time\": feat_time,\n",
        "    \"feat_dsc\": feat_dsc,\n",
        "    \"feat_current_sensor\": feat_current_sensor,\n",
        "    \"feat_current_ESM\": feat_current_ESM,\n",
        "    \"feat_ImmediatePast_sensor\": feat_ImmediatePast_sensor,\n",
        "    \"feat_ImmediatePast_ESM\": feat_ImmediatePast_ESM,\n",
        "    \"feat_today_sensor\": feat_today_sensor,\n",
        "    \"feat_today_ESM\": feat_today_ESM,\n",
        "    \"feat_yesterday_sensor\": feat_yesterday_sensor,\n",
        "    \"feat_yesterday_ESM\": feat_yesterday_ESM,\n",
        "    \"feat_sleep\": feat_sleep,\n",
        "    \"feat_pif\": feat_pif,\n",
        "}\n",
        "\n",
        "feature_summary = {name: data.shape[1] for name, data in feature_groups.items()}\n",
        "feature_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PSKfANO9T8b",
        "outputId": "81704487-aa94-4a29-f7e8-80009399a2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'feat_time': 16,\n",
              " 'feat_dsc': 55,\n",
              " 'feat_current_sensor': 84,\n",
              " 'feat_current_ESM': 1,\n",
              " 'feat_ImmediatePast_sensor': 416,\n",
              " 'feat_ImmediatePast_ESM': 0,\n",
              " 'feat_today_sensor': 2496,\n",
              " 'feat_today_ESM': 6,\n",
              " 'feat_yesterday_sensor': 2496,\n",
              " 'feat_yesterday_ESM': 6,\n",
              " 'feat_sleep': 2,\n",
              " 'feat_pif': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Model Training & Evaluation\n"
      ],
      "metadata": {
        "id": "kPIZll5fXQld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the revised XGBoost Classifier. We will use random eval_size percent of training set data as evaluation set for early stoppping."
      ],
      "metadata": {
        "id": "2sE6PaldpCNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier, DMatrix\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from typing import Union\n",
        "\n",
        "#Function for revised xgboost classifier\n",
        "class EvXGBClassifier(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Enhanced XGBClassifier with built-in validation set approach for early stopping.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_size=None,\n",
        "        eval_metric='logloss',\n",
        "        early_stopping_rounds=10,\n",
        "        random_state=None,\n",
        "        **kwargs\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initializes the custom XGBoost Classifier.\n",
        "\n",
        "        Args:\n",
        "            eval_size (float): The proportion of the dataset to include in the evaluation split.\n",
        "            eval_metric (str): The evaluation metric used for model training.\n",
        "            early_stopping_rounds (int): The number of rounds to stop training if hold-out metric doesn't improve.\n",
        "            random_state (int): Seed for the random number generator for reproducibility.\n",
        "            **kwargs: Additional arguments to be passed to the underlying XGBClassifier.\n",
        "        \"\"\"\n",
        "        self.random_state = random_state\n",
        "        self.eval_size = eval_size\n",
        "        self.eval_metric = eval_metric\n",
        "        self.early_stopping_rounds = early_stopping_rounds\n",
        "        # Initialize the XGBClassifier with specified arguments and GPU acceleration.\n",
        "        self.model = XGBClassifier(\n",
        "            random_state=self.random_state,\n",
        "            eval_metric=self.eval_metric,\n",
        "            early_stopping_rounds=self.early_stopping_rounds,\n",
        "            tree_method = \"hist\", device = \"cuda\", #Use gpu for acceleration\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        \"\"\" Returns the feature importances from the fitted model. \"\"\"\n",
        "        return self.model.feature_importances_\n",
        "\n",
        "    @property\n",
        "    def feature_names_in_(self):\n",
        "        \"\"\" Returns the feature names from the input dataset used for fitting. \"\"\"\n",
        "        return self.model.feature_names_in_\n",
        "\n",
        "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the XGBoost model with optional early stopping using a validation set.\n",
        "\n",
        "        Args:\n",
        "            X (Union[pd.DataFrame, np.ndarray]): Training features.\n",
        "            y (np.ndarray): Target values.\n",
        "        \"\"\"\n",
        "        if self.eval_size:\n",
        "            # Split data for early stopping evaluation if eval_size is specified.\n",
        "            X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
        "                X, y, test_size=self.eval_size, random_state=self.random_state)\n",
        "            # Fit the model with early stopping.\n",
        "            self.model.fit(\n",
        "                X_train_sub, y_train_sub,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "        else:\n",
        "            # Fit the model without early stopping.\n",
        "            self.model.set_params(early_stopping_rounds=None)\n",
        "            self.model.fit(X, y, verbose=False)\n",
        "\n",
        "        # Store the best iteration number for predictions.\n",
        "        # Best iteration (safe fallback)\n",
        "        booster = self.model.get_booster()\n",
        "        self.best_iteration_ = (\n",
        "            booster.best_iteration if hasattr(booster, \"best_iteration\") else None\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the classes for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        if self.best_iteration_ is not None:\n",
        "            return self.model.predict(X, iteration_range=(0, self.best_iteration_ + 1))\n",
        "        else:\n",
        "            return self.model.predict(X)\n",
        "\n",
        "    def predict_proba(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the class probabilities for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        if self.best_iteration_ is not None:\n",
        "            return self.model.predict_proba(X, iteration_range=(0, self.best_iteration_ + 1))\n",
        "        else:\n",
        "            return self.model.predict_proba(X)"
      ],
      "metadata": {
        "id": "cxqMVtSVXTfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is defined functions for model training and model evaluation (cross-validation)."
      ],
      "metadata": {
        "id": "8h_yate5pYRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import traceback\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class FoldResult:\n",
        "    name: str\n",
        "    metrics: dict\n",
        "    duration: float\n",
        "\n",
        "def log(message: str):\n",
        "    print(message)  # Simple logging to stdout or enhance as needed\n",
        "\n",
        "def train_fold(dir_result: str, fold_name: str, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state):\n",
        "    \"\"\"\n",
        "    Function to train and evaluate the model for a single fold.\n",
        "    Args:\n",
        "        dir_result (str): Directory to store results.\n",
        "        fold_name (str): Name of the fold for identification.\n",
        "        X_train, y_train (DataFrame, Series): Training data.\n",
        "        X_test, y_test (DataFrame, Series): Testing data.\n",
        "        C_cat, C_num (array): Lists of categorical and numeric feature names.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize (bool): Flag to apply normalization.\n",
        "        select (SelectFromModel instance): Feature selection method.\n",
        "        oversample (bool): Flag to apply oversampling.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "    Returns:\n",
        "        FoldResult: Object containing metrics and duration of the training.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        if normalize:\n",
        "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "            # Standard scaler only applied to numeric data\n",
        "            scaler = StandardScaler().fit(X_train_N)\n",
        "            X_train_N = scaler.transform(X_train_N)\n",
        "            X_test_N = scaler.transform(X_test_N)\n",
        "\n",
        "            X_train = pd.DataFrame(\n",
        "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "            X_test = pd.DataFrame(\n",
        "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "\n",
        "        #Applying the LASSO feature selection method\n",
        "        if select:\n",
        "\n",
        "            if isinstance(select, SelectFromModel):\n",
        "                select = [select]\n",
        "\n",
        "            for i, s in enumerate(select):\n",
        "                C = np.asarray(X_train.columns)\n",
        "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
        "                C_sel = C[M]\n",
        "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
        "                C_num = C_num[np.isin(C_num, C_sel)]\n",
        "\n",
        "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "\n",
        "\n",
        "                X_train = pd.DataFrame(\n",
        "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "                X_test = pd.DataFrame(\n",
        "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "\n",
        "        if oversample:\n",
        "            #If there is any categorical data, apply SMOTE-NC, otherwise just SMOTE\n",
        "            if len(C_cat) > 0:\n",
        "                sampler = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in C_cat], random_state=random_state)\n",
        "            else:\n",
        "                sampler = SMOTE(random_state=random_state)\n",
        "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        estimator = clone(estimator).fit(X_train, y_train)\n",
        "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
        "        #Deafult average method for roc_auc_score is macro\n",
        "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
        "\n",
        "        result = FoldResult(\n",
        "            name=fold_name,\n",
        "            metrics={'AUC': auc_score},\n",
        "            duration=time.time() - start_time\n",
        "        )\n",
        "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
        "        return None\n",
        "\n",
        "#We modify to include tge category information in to this function\n",
        "def perform_cross_validation(X, y, groups, estimator, normalize=False, select=None, oversample=False, random_state=None):\n",
        "    \"\"\"\n",
        "    Function to perform cross-validation using StratifiedGroupKFold.\n",
        "    Args:\n",
        "        X, y (DataFrame, Series): The entire dataset.\n",
        "        groups (array): Array indicating the group for each instance in X.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize, select, oversample (bool): Preprocessing options.\n",
        "        random_state (int): Seed for reproducibility.\n",
        "    Returns:\n",
        "        list: A list containing FoldResult for each fold.\n",
        "    \"\"\"\n",
        "    futures = []\n",
        "    # Group-k cross validation\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle =True, random_state = 42)\n",
        "    # Loop over all the LOSO splits\n",
        "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "        C_cat = np.asarray(sorted(cats))\n",
        "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
        "\n",
        "        job = train_fold('path_to_results', f'Fold_{idx}', X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
        "        futures.append(job)\n",
        "\n",
        "    return futures"
      ],
      "metadata": {
        "id": "AoHTuB3qpsfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define the feature selection method and classifier and execute the code. AUC-ROC is calculated as mean of macro AUC-ROC for all folds/users."
      ],
      "metadata": {
        "id": "ZOD1KJTup7il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Featur Selection, you may want to change the feature selection methods\n",
        "SELECT_LASSO = SelectFromModel(\n",
        "        estimator=LogisticRegression(\n",
        "        penalty='l1'\n",
        "        ,solver='liblinear'\n",
        "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
        "    ),\n",
        "    # This threshold may impact the model performance as well\n",
        "    threshold = 0.005\n",
        ")\n",
        "#Classifier\n",
        "#There could exist more parameters. Please search in your defined parameter\n",
        "#space for model performance improvement\n",
        "estimator = EvXGBClassifier(\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic', #Prediction instead of regression\n",
        "    verbosity=0,\n",
        "    learning_rate=0.01,\n",
        ")\n",
        "\n",
        "#Perform cross validation including model training and evaluation\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqvkNsznrEDe",
        "outputId": "cad268a5-f27e-4e76-d794-bbd5ebfc7879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:50:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for Fold_0 with AUC: 0.5760597892673365\n",
            "Training completed for Fold_1 with AUC: 0.5194843617920542\n",
            "Training completed for Fold_2 with AUC: 0.605396012438266\n",
            "Training completed for Fold_3 with AUC: 0.5232254989908052\n",
            "Training completed for Fold_4 with AUC: 0.5972818082858423\n",
            "0.5642894941548608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment"
      ],
      "metadata": {
        "id": "O-EZcuiA56Ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 1. Improve the model performance using different types of feature combinations. (20pts)"
      ],
      "metadata": {
        "id": "fuOqHMwFzWHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Hint: Currently we are only using feat_baseline. You may want to try other feature combinations."
      ],
      "metadata": {
        "id": "6kR-jSPN0i52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Refactorization\n",
        "\n",
        "First we define a function for feature extraction from the dataset. We follow almost all of the categorizations from the above given skeleton, but we just put them in a function for reuse in future.\n",
        "\n",
        "Secondly, we also refactor the train_fold and the perform_cross_validation functions to include benchmark information for multiple features combinations, which will be more understandable in the experiment section.\n"
      ],
      "metadata": {
        "id": "jf3V9AML6fCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_and_split_features(X, y, groups, datetimes):\n",
        "    \"\"\"\n",
        "    Reorder data chronologically and extract all categorized features.\n",
        "\n",
        "    Parameters:\n",
        "    X : raw features pickle loaded from the dataset\n",
        "    y : Target labels.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({'user_id': groups, 'datetime': datetimes, 'label': y})\n",
        "    df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "    df_merged = df_merged.sort_values(by=['user_id', 'datetime'])\n",
        "\n",
        "    groups = df_merged['user_id'].to_numpy()\n",
        "    datetimes = df_merged['datetime'].to_numpy()\n",
        "    y = df_merged['label'].to_numpy()\n",
        "    X_cleaned = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
        "\n",
        "    # Categorized features\n",
        "    feat_current = X_cleaned.loc[:, [('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X_cleaned.columns]]\n",
        "    feat_dsc = X_cleaned.loc[:, [('#DSC' in str(x)) for x in X_cleaned.columns]]\n",
        "    feat_yesterday = X_cleaned.loc[:, [('Yesterday' in str(x)) for x in X_cleaned.columns]]\n",
        "    feat_today = X_cleaned.loc[:, [('Today' in str(x)) for x in X_cleaned.columns]]\n",
        "    feat_ImmediatePast = X_cleaned.loc[:, [('ImmediatePast_15' in str(x)) for x in X_cleaned.columns]]\n",
        "\n",
        "    # Fine-grained subcategories\n",
        "    feat_current_sensor = X_cleaned.loc[:, [('#VAL' in str(x)) for x in X_cleaned.columns]]\n",
        "    feat_current_ESM = X_cleaned.loc[:, [('ESM#LastLabel' in str(x)) for x in X_cleaned.columns]]\n",
        "    feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:, [('ESM' not in str(x)) for x in feat_ImmediatePast.columns]]\n",
        "    feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:, [('ESM' in str(x)) for x in feat_ImmediatePast.columns]]\n",
        "    feat_today_sensor = feat_today.loc[:, [('ESM' not in str(x)) for x in feat_today.columns]]\n",
        "    feat_today_ESM = feat_today.loc[:, [('ESM' in str(x)) for x in feat_today.columns]]\n",
        "    feat_yesterday_sensor = feat_yesterday.loc[:, [('ESM' not in str(x)) for x in feat_yesterday.columns]]\n",
        "    feat_yesterday_ESM = feat_yesterday.loc[:, [('ESM' in str(x)) for x in feat_yesterday.columns]]\n",
        "\n",
        "    feat_sleep = X_cleaned.loc[:, [('Sleep' in str(x)) for x in X_cleaned.columns]]\n",
        "    feat_time = X_cleaned.loc[:, [('Time' in str(x)) for x in X_cleaned.columns]]\n",
        "    feat_pif = X_cleaned.loc[:, [('PIF' in str(x)) for x in X_cleaned.columns]]\n",
        "\n",
        "    # Baseline feature combination\n",
        "    feat_baseline = pd.concat([feat_time, feat_dsc, feat_current_sensor, feat_ImmediatePast_sensor], axis=1)\n",
        "\n",
        "    return {\n",
        "        \"X_cleaned\": X_cleaned,\n",
        "        \"y\": y,\n",
        "        \"groups\": groups,\n",
        "        \"datetimes\": datetimes,\n",
        "        \"feat_current\": feat_current,\n",
        "        \"feat_dsc\": feat_dsc,\n",
        "        \"feat_yesterday\": feat_yesterday,\n",
        "        \"feat_today\": feat_today,\n",
        "        \"feat_ImmediatePast\": feat_ImmediatePast,\n",
        "        \"feat_current_sensor\": feat_current_sensor,\n",
        "        \"feat_current_ESM\": feat_current_ESM,\n",
        "        \"feat_ImmediatePast_sensor\": feat_ImmediatePast_sensor,\n",
        "        \"feat_ImmediatePast_ESM\": feat_ImmediatePast_ESM,\n",
        "        \"feat_today_sensor\": feat_today_sensor,\n",
        "        \"feat_today_ESM\": feat_today_ESM,\n",
        "        \"feat_yesterday_sensor\": feat_yesterday_sensor,\n",
        "        \"feat_yesterday_ESM\": feat_yesterday_ESM,\n",
        "        \"feat_sleep\": feat_sleep,\n",
        "        \"feat_time\": feat_time,\n",
        "        \"feat_pif\": feat_pif,\n",
        "        \"feat_baseline\": feat_baseline\n",
        "    }\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FoldResult:\n",
        "    \"\"\"\n",
        "    Data class to store results for each fold.\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    metrics: dict\n",
        "    duration: float\n",
        "\n",
        "def log(message: str):\n",
        "    \"\"\"\n",
        "    Simple logger function.\n",
        "    \"\"\"\n",
        "    print(message)\n",
        "\n",
        "def train_fold(fold_name, X_train, y_train, X_test, y_test, C_cat, C_num,\n",
        "               estimator, normalize, select, oversample, random_state):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a model on a single fold.\n",
        "\n",
        "    Args:\n",
        "        fold_name: Name of the fold.\n",
        "        X_train, y_train: Training data and labels.\n",
        "        X_test, y_test: Test data and labels.\n",
        "        C_cat: List of categorical feature names.\n",
        "        C_num: List of numerical feature names.\n",
        "        estimator: Model to train.\n",
        "        normalize: Whether to normalize numeric features.\n",
        "        select: Feature selector(s).\n",
        "        oversample: Whether to apply oversampling.\n",
        "        random_state: Random seed.\n",
        "\n",
        "    Returns:\n",
        "        FoldResult object with metrics and duration.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Normalize numeric features if requested\n",
        "        if normalize:\n",
        "            X_train_N = X_train[C_num].values\n",
        "            X_test_N = X_test[C_num].values\n",
        "            X_train_C = X_train[C_cat].values\n",
        "            X_test_C = X_test[C_cat].values\n",
        "\n",
        "            scaler = StandardScaler().fit(X_train_N)\n",
        "            X_train_N = scaler.transform(X_train_N)\n",
        "            X_test_N = scaler.transform(X_test_N)\n",
        "\n",
        "            # Concatenate categorical and normalized numeric features\n",
        "            X_train = pd.DataFrame(\n",
        "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "            X_test = pd.DataFrame(\n",
        "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "\n",
        "        # Feature selection if requested\n",
        "        if select:\n",
        "            if isinstance(select, SelectFromModel):\n",
        "                select = [select]\n",
        "\n",
        "            for s in select:\n",
        "                support = s.fit(X_train.values, y_train).get_support()\n",
        "                selected_cols = X_train.columns[support]\n",
        "                C_cat = np.intersect1d(C_cat, selected_cols)\n",
        "                C_num = np.intersect1d(C_num, selected_cols)\n",
        "\n",
        "                X_train = X_train[selected_cols]\n",
        "                X_test = X_test[selected_cols]\n",
        "\n",
        "        # Oversampling if requested\n",
        "        if oversample:\n",
        "            if len(C_cat) > 0:\n",
        "                sampler = SMOTENC(\n",
        "                    categorical_features=[X_train.columns.get_loc(c) for c in C_cat],\n",
        "                    random_state=random_state\n",
        "                )\n",
        "            else:\n",
        "                sampler = SMOTE(random_state=random_state)\n",
        "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        # Train the estimator and compute AUC\n",
        "        estimator = clone(estimator).fit(X_train, y_train)\n",
        "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
        "        auc_score = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "        return FoldResult(\n",
        "            name=fold_name,\n",
        "            metrics={'AUC': auc_score},\n",
        "            duration=time.time() - start_time\n",
        "        )\n",
        "\n",
        "    except Exception:\n",
        "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
        "        return None\n",
        "\n",
        "def perform_cross_validation(X, y, groups, estimator, cats, normalize=False, select=None, oversample=False, random_state=None):\n",
        "    \"\"\"\n",
        "    Performs cross-validation using StratifiedGroupKFold.\n",
        "\n",
        "    Args:\n",
        "        X: Feature DataFrame.\n",
        "        y: Target array.\n",
        "        groups: Group labels for the samples.\n",
        "        estimator: Model to train.\n",
        "        cats: List of categorical feature names.\n",
        "        normalize: Whether to normalize numeric features.\n",
        "        select: Feature selector(s).\n",
        "        oversample: Whether to apply oversampling.\n",
        "        random_state: Random seed.\n",
        "\n",
        "    Returns:\n",
        "        List of FoldResult objects.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    #We use the StratifiedGroupKFoldn for validation\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Separate categorical and numeric columns\n",
        "        C_cat = np.asarray(sorted(cats))\n",
        "        C_num = np.asarray([col for col in X.columns if col not in C_cat])\n",
        "\n",
        "        result = train_fold(f\"Fold_{idx}\", X_train, y_train, X_test, y_test,\n",
        "                            C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
        "        if result:\n",
        "            results.append(result)\n",
        "\n",
        "    # Print AUC for each fold\n",
        "    for res in results:\n",
        "        log(f\"{res.name} - AUC: {res.metrics['AUC']:.4f} | Duration: {res.duration:.2f}s\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "wD6HOgC_zuiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Selection Experiment\n",
        "\n",
        "We define feature combinations and put them in dataframe for efficient experiment."
      ],
      "metadata": {
        "id": "hJzJ4LZ17Hqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reload the dataset\n",
        "X_raw, y, groups, t, datetimes = pickle.load(open(PATH, mode='rb'))\n",
        "\n",
        "#Feature categorization and sorting for ease-access to the features later on.\n",
        "features = reorder_and_split_features(X_raw, y, groups, datetimes)\n",
        "\n",
        "\n",
        "#Selecting combination of features set for running the experiment\n",
        "feature_sets = {\n",
        "    \"baseline\": features[\"feat_baseline\"],\n",
        "\n",
        "    # all sensors\n",
        "    \"all_sensors\": features[\"X_cleaned\"],\n",
        "\n",
        "\n",
        "    # baseline + feat_today_sensor(2496), feat_today_ESM (6), feat_current_ESM (1) +  feat_pid (1) + feat_sleep (1)\n",
        "    \"baseline+today+current_esm+pid+sleep\" : features[\"feat_baseline\"]\n",
        "        .join(features[\"feat_today_sensor\"])\n",
        "        .join(features[\"feat_today_ESM\"])\n",
        "        .join(features[\"feat_current_ESM\"])\n",
        "        .join(features[\"feat_pif\"])\n",
        "        .join(features[\"feat_sleep\"]),\n",
        "\n",
        "    # baseline + feat_today_sensor(2496), feat_today_ESM (6), feat_current_ESM (1) + feat_sleep (1)\n",
        "    \"baseline+today+current_esm+sleep\" : features[\"feat_baseline\"]\n",
        "        .join(features[\"feat_today_sensor\"])\n",
        "        .join(features[\"feat_today_ESM\"])\n",
        "        .join(features[\"feat_current_ESM\"])\n",
        "        .join(features[\"feat_sleep\"]),\n",
        "\n",
        "    # baseline + baseline+today+current_esm + feat_yesterday_sensor (2496) + feat_yesterday_ESM (6) + immediate_past_ESM (6)\n",
        "    \"baseline+today+current_esm+yesterday+immediate_past_esm\" : features[\"feat_baseline\"]\n",
        "        .join(features[\"feat_today_sensor\"])\n",
        "        .join(features[\"feat_today_ESM\"])\n",
        "        .join(features[\"feat_current_ESM\"])\n",
        "        .join(features[\"feat_yesterday_sensor\"])\n",
        "        .join(features[\"feat_yesterday_ESM\"])\n",
        "        .join(features[\"feat_ImmediatePast_ESM\"]),\n",
        "\n",
        "    # baseline + baseline+today+current_esm + feat_yesterday_sensor (2496) + feat_yesterday_ESM (6) + feat_pid (1) + feat_sleep (1)\n",
        "    \"baseline+today+current_esm+yesterday+pid+sleep\" : features[\"feat_baseline\"]\n",
        "        .join(features[\"feat_today_sensor\"])\n",
        "        .join(features[\"feat_today_ESM\"])\n",
        "        .join(features[\"feat_current_ESM\"])\n",
        "        .join(features[\"feat_yesterday_sensor\"])\n",
        "        .join(features[\"feat_yesterday_ESM\"])\n",
        "        .join(features[\"feat_pif\"])\n",
        "        .join(features[\"feat_sleep\"]),\n",
        "\n",
        "    #baseline + baseline+today+current_esm + feat_yesterday_sensor (2496) + feat_yesterday_ESM (6) - immediate_past_sensor (2496)(remove from baseline)\n",
        "    \"baseline+today+current_esm+yesterday-no_immediate_past\" : features[\"feat_baseline\"]\n",
        "        .join(features[\"feat_today_sensor\"])\n",
        "        .join(features[\"feat_today_ESM\"])\n",
        "        .join(features[\"feat_current_ESM\"])\n",
        "        .join(features[\"feat_yesterday_sensor\"])\n",
        "        .join(features[\"feat_yesterday_ESM\"])\n",
        "        .drop(columns=features[\"feat_ImmediatePast_sensor\"].columns),\n",
        "\n",
        "    # baseline + baseline+today+current_esm + feat_yesterday_sensor (2496) + feat_yesterday_ESM (6)\n",
        "    \"baseline+today+current_esm+yesterday\" : features[\"feat_baseline\"]\n",
        "        .join(features[\"feat_today_sensor\"])\n",
        "        .join(features[\"feat_today_ESM\"])\n",
        "        .join(features[\"feat_current_ESM\"])\n",
        "        .join(features[\"feat_yesterday_sensor\"])\n",
        "        .join(features[\"feat_yesterday_ESM\"]),\n",
        "\n",
        "\n",
        "    # baseline + feat_today_sensor(2496), feat_today_ESM (6), feat_current_ESM (1)\n",
        "    \"baseline+today+current_esm\" : features[\"feat_baseline\"]\n",
        "        .join(features[\"feat_today_sensor\"])\n",
        "        .join(features[\"feat_today_ESM\"])\n",
        "        .join(features[\"feat_current_ESM\"]),\n",
        "\n",
        "    # current+ImmediatePast\n",
        "    \"current+ImmediatePast\": features[\"feat_current_sensor\"]\n",
        "        .join(features[\"feat_ImmediatePast_sensor\"]),\n",
        "\n",
        "    # current\n",
        "    \"current\": features[\"feat_current\"],\n",
        "\n",
        "    # dsc\n",
        "    \"dsc\": features[\"feat_dsc\"],\n",
        "\n",
        "    # sensor+time\n",
        "    \"sensor+time\": features[\"feat_current_sensor\"].join(features[\"feat_time\"]),\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKRHbv9Q7Mpa",
        "outputId": "88bb992b-ccb9-4fcc-eb6f-c11df68c4203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-3886126501>:2: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  X_raw, y, groups, t, datetimes = pickle.load(open(PATH, mode='rb'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running the experiment"
      ],
      "metadata": {
        "id": "nHuzCNjA7_y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, X_feat in feature_sets.items():\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "\n",
        "    # Use boolean columns as categorical features\n",
        "    cat_cols = X_feat.columns[X_feat.dtypes == bool]\n",
        "\n",
        "    # Running the Validation\n",
        "    results = perform_cross_validation(\n",
        "        X_feat, features[\"y\"], features[\"groups\"],\n",
        "        estimator=estimator,\n",
        "        cats=cat_cols,\n",
        "        normalize=True,\n",
        "        select=[SELECT_LASSO],\n",
        "        oversample=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    auc_scores = [r.metrics[\"AUC\"] for r in results]\n",
        "    avg_auc = np.mean(auc_scores)\n",
        "\n",
        "    print(f\"Mean AUC: {avg_auc:.4f}\")\n",
        "\n",
        "    # Save results\n",
        "    pd.DataFrame({\n",
        "        \"AUC Scores\": auc_scores,\n",
        "        \"Mean AUC\": [avg_auc] * len(auc_scores)\n",
        "    }).to_csv(f\"assignment1_{name}_cv_results.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7GjAr1671-Z",
        "outputId": "843f12a7-23e8-4b0f-f18a-3f6aced65f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- baseline ---\n",
            "Fold_0 - AUC: 0.6111 | Duration: 6.07s\n",
            "Fold_1 - AUC: 0.5195 | Duration: 4.46s\n",
            "Fold_2 - AUC: 0.5945 | Duration: 4.92s\n",
            "Fold_3 - AUC: 0.5064 | Duration: 5.43s\n",
            "Fold_4 - AUC: 0.5734 | Duration: 4.20s\n",
            "Mean AUC: 0.5610\n",
            "\n",
            "--- all_sensors ---\n",
            "Fold_0 - AUC: 0.5982 | Duration: 12.93s\n",
            "Fold_1 - AUC: 0.5827 | Duration: 9.47s\n",
            "Fold_2 - AUC: 0.5949 | Duration: 12.29s\n",
            "Fold_3 - AUC: 0.5330 | Duration: 11.02s\n",
            "Fold_4 - AUC: 0.5828 | Duration: 12.04s\n",
            "Mean AUC: 0.5783\n",
            "\n",
            "--- baseline+today+current_esm+pid+sleep ---\n",
            "Fold_0 - AUC: 0.6194 | Duration: 14.46s\n",
            "Fold_1 - AUC: 0.5768 | Duration: 11.30s\n",
            "Fold_2 - AUC: 0.5850 | Duration: 14.30s\n",
            "Fold_3 - AUC: 0.5355 | Duration: 10.81s\n",
            "Fold_4 - AUC: 0.5917 | Duration: 13.36s\n",
            "Mean AUC: 0.5817\n",
            "\n",
            "--- baseline+today+current_esm+sleep ---\n",
            "Fold_0 - AUC: 0.5810 | Duration: 15.67s\n",
            "Fold_1 - AUC: 0.5318 | Duration: 10.95s\n",
            "Fold_2 - AUC: 0.6411 | Duration: 14.41s\n",
            "Fold_3 - AUC: 0.5230 | Duration: 12.16s\n",
            "Fold_4 - AUC: 0.6184 | Duration: 12.61s\n",
            "Mean AUC: 0.5790\n",
            "\n",
            "--- baseline+today+current_esm+yesterday+immediate_past_esm ---\n",
            "Fold_0 - AUC: 0.6280 | Duration: 12.94s\n",
            "Fold_1 - AUC: 0.6216 | Duration: 9.95s\n",
            "Fold_2 - AUC: 0.5959 | Duration: 12.20s\n",
            "Fold_3 - AUC: 0.5462 | Duration: 11.04s\n",
            "Fold_4 - AUC: 0.6040 | Duration: 10.96s\n",
            "Mean AUC: 0.5991\n",
            "\n",
            "--- baseline+today+current_esm+yesterday+pid+sleep ---\n",
            "Fold_0 - AUC: 0.6159 | Duration: 12.77s\n",
            "Fold_1 - AUC: 0.5539 | Duration: 9.96s\n",
            "Fold_2 - AUC: 0.5759 | Duration: 12.65s\n",
            "Fold_3 - AUC: 0.5676 | Duration: 11.30s\n",
            "Fold_4 - AUC: 0.6140 | Duration: 11.83s\n",
            "Mean AUC: 0.5855\n",
            "\n",
            "--- baseline+today+current_esm+yesterday-no_immediate_past ---\n",
            "Fold_0 - AUC: 0.6044 | Duration: 14.98s\n",
            "Fold_1 - AUC: 0.5934 | Duration: 12.10s\n",
            "Fold_2 - AUC: 0.5670 | Duration: 12.91s\n",
            "Fold_3 - AUC: 0.5653 | Duration: 11.80s\n",
            "Fold_4 - AUC: 0.6356 | Duration: 12.35s\n",
            "Mean AUC: 0.5932\n",
            "\n",
            "--- baseline+today+current_esm+yesterday ---\n",
            "Fold_0 - AUC: 0.6280 | Duration: 13.31s\n",
            "Fold_1 - AUC: 0.6216 | Duration: 9.80s\n",
            "Fold_2 - AUC: 0.5959 | Duration: 12.43s\n",
            "Fold_3 - AUC: 0.5462 | Duration: 11.12s\n",
            "Fold_4 - AUC: 0.6040 | Duration: 10.99s\n",
            "Mean AUC: 0.5991\n",
            "\n",
            "--- baseline+today+current_esm ---\n",
            "Fold_0 - AUC: 0.5987 | Duration: 16.35s\n",
            "Fold_1 - AUC: 0.5871 | Duration: 11.29s\n",
            "Fold_2 - AUC: 0.6006 | Duration: 14.47s\n",
            "Fold_3 - AUC: 0.5321 | Duration: 12.67s\n",
            "Fold_4 - AUC: 0.6303 | Duration: 13.13s\n",
            "Mean AUC: 0.5898\n",
            "\n",
            "--- current+ImmediatePast ---\n",
            "Fold_0 - AUC: 0.5902 | Duration: 3.17s\n",
            "Fold_1 - AUC: 0.5255 | Duration: 4.72s\n",
            "Fold_2 - AUC: 0.5634 | Duration: 4.56s\n",
            "Fold_3 - AUC: 0.5342 | Duration: 3.80s\n",
            "Fold_4 - AUC: 0.5977 | Duration: 5.52s\n",
            "Mean AUC: 0.5622\n",
            "\n",
            "--- current ---\n",
            "Fold_0 - AUC: 0.5806 | Duration: 0.77s\n",
            "Fold_1 - AUC: 0.5779 | Duration: 1.08s\n",
            "Fold_2 - AUC: 0.5384 | Duration: 1.19s\n",
            "Fold_3 - AUC: 0.5556 | Duration: 0.56s\n",
            "Fold_4 - AUC: 0.6276 | Duration: 0.96s\n",
            "Mean AUC: 0.5760\n",
            "\n",
            "--- dsc ---\n",
            "Fold_0 - AUC: 0.5649 | Duration: 1.00s\n",
            "Fold_1 - AUC: 0.5072 | Duration: 0.97s\n",
            "Fold_2 - AUC: 0.5327 | Duration: 1.22s\n",
            "Fold_3 - AUC: 0.5664 | Duration: 1.03s\n",
            "Fold_4 - AUC: 0.5281 | Duration: 0.92s\n",
            "Mean AUC: 0.5399\n",
            "\n",
            "--- sensor+time ---\n",
            "Fold_0 - AUC: 0.5220 | Duration: 0.96s\n",
            "Fold_1 - AUC: 0.5660 | Duration: 1.21s\n",
            "Fold_2 - AUC: 0.5021 | Duration: 1.06s\n",
            "Fold_3 - AUC: 0.5611 | Duration: 0.83s\n",
            "Fold_4 - AUC: 0.5402 | Duration: 0.99s\n",
            "Mean AUC: 0.5383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 1: Discussion\n",
        "\n",
        "The feature combinations were at first selected based on intuitions. However, it was observed that the baseline feature set did not have the ESM data at all. Therefore, as a first step, feature sets with the esm data were tested and a performance improvement was observed.\n",
        "\n",
        "One thing to note that the sleep data and thermal data (dsc) did not have much impact on the results both as standalone and when combined with other features."
      ],
      "metadata": {
        "id": "L5I3BcpwBj0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 2. Please try different feature selection methods (20pts)"
      ],
      "metadata": {
        "id": "TMEhtgwJz9IP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: Currently, we are using LASSO filter for feature selection. Please consider using embedded method as well(same model for both feature selection and model training). Besides, the threshold for LASSO filter may also affect the performance. **Sepcifically, there is a method called 'mean' which is using mean of feature importances of all features as threshold.** Please try both different feature selection methods and different thresholds for filtering features to improve model performance."
      ],
      "metadata": {
        "id": "8FCRh8eF0zud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We adopt the following three methods for feature selections:\n",
        "\n",
        "\n",
        "*   LASSO (Linear Model): Used in the baseline; we select feature sets with AUC  0.579 from the baseline feature combination experiments.\n",
        "*   SHAP (Model-Agnostic):  All features were used to avoid pre filtering features; SHAP captures global and local importance\n",
        "* Random Forest (Non-Linear Model): All features included; RF ranks features based on non-linear interactions.\n",
        "\n",
        " These are not just three methods, buy they represent three fundamentally different types of feature selection approaches, which we thought to be interesting to experiment with."
      ],
      "metadata": {
        "id": "LjOvkQY7Dpiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LASSO Selection Approach"
      ],
      "metadata": {
        "id": "yEk_orUZH4r4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select feature sets with AUC  0.579 from the baseline feature combination experiments.\n",
        "\n"
      ],
      "metadata": {
        "id": "qx4QzUMoH9LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_sets = {\n",
        "    \"feat_baseline\": features['feat_baseline'],\n",
        "\n",
        "    \"baseline+today+current_esm\": features['feat_baseline']\n",
        "        .join(features['feat_today_sensor'])\n",
        "        .join(features['feat_today_ESM'])\n",
        "        .join(features['feat_current_ESM']),\n",
        "\n",
        "    \"baseline+today+current_esm+yesterday\": features['feat_baseline']\n",
        "        .join(features['feat_today_sensor'])\n",
        "        .join(features['feat_today_ESM'])\n",
        "        .join(features['feat_current_ESM'])\n",
        "        .join(features['feat_yesterday_sensor'])\n",
        "        .join(features['feat_yesterday_ESM']),\n",
        "\n",
        "    \"baseline+today+current_esm+yesterday-no_immediate_past\": features['feat_baseline']\n",
        "        .join(features['feat_today_sensor'])\n",
        "        .join(features['feat_today_ESM'])\n",
        "        .join(features['feat_current_ESM'])\n",
        "        .join(features['feat_yesterday_sensor'])\n",
        "        .join(features['feat_yesterday_ESM'])\n",
        "        .drop(columns=features['feat_ImmediatePast_sensor'].columns),\n",
        "\n",
        "    \"baseline+today+current_esm+yesterday+immediate_past_esm\": features['feat_baseline']\n",
        "        .join(features['feat_today_sensor'])\n",
        "        .join(features['feat_today_ESM'])\n",
        "        .join(features['feat_current_ESM'])\n",
        "        .join(features['feat_yesterday_sensor'])\n",
        "        .join(features['feat_yesterday_ESM'])\n",
        "        .join(features['feat_ImmediatePast_ESM']),\n",
        "}"
      ],
      "metadata": {
        "id": "w1VNqRfEI3eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform **LASSO-based feature selection** using different configurations of:\n",
        "\n",
        "- **C values**: `[0.1, 1.0, 10.0]`  controls the strength of regularization.\n",
        "- **Thresholds**: `[0.001, 0.005, 'mean']`  controls how strict the feature selection is.\n",
        "\n",
        "We create a list of `SelectFromModel` selectors by pairing each C value with each threshold.  \n",
        "These help us explore how different levels of sparsity impact model performance.\n",
        "\n",
        "After selecting features, we use the consistent downstream model (`EvXGBClassifier`) to evaluate performance.\n",
        "\n",
        "The aim is to identify which LASSO configuration best balances simplicity (fewer features) with predictive performance."
      ],
      "metadata": {
        "id": "Fn_8uviYJ8-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying different regularization strengths for LASSO (low to high)\n",
        "C_values = [0.1, 1.0, 10.0]\n",
        "\n",
        "# Trying different thresholds to control feature selection sparsity\n",
        "thresholds = [0.001, 0.005, 'mean']\n",
        "\n",
        "# Setting up multiple LASSO selectors with different C and threshold combinations\n",
        "# This helps us tune how strict or loose feature selection should be\n",
        "selectors = [\n",
        "    SelectFromModel(\n",
        "        estimator=LogisticRegression(\n",
        "            penalty='l1',            # LASSO penalty to shrink irrelevant features\n",
        "            solver='liblinear',      # Works with L1 penalty\n",
        "            C=c,                     # Controls regularization strength\n",
        "            random_state=42,\n",
        "            max_iter=4000            # Ensure the model converges\n",
        "        ),\n",
        "        threshold=thresh            # Controls which features are kept\n",
        "    )\n",
        "    for c in C_values\n",
        "    for thresh in thresholds\n",
        "]\n",
        "\n",
        "# Classifier used after LASSO-based feature selection\n",
        "# EvXGBClassifier = XGBoost-based model used for final predictions\n",
        "estimator = EvXGBClassifier(\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic',\n",
        "    verbosity=0,\n",
        "    learning_rate=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "Bwna8wpv3Dhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fs_name, X_feat in feature_sets.items():\n",
        "    cat_cols = X_feat.columns[X_feat.dtypes == bool]\n",
        "\n",
        "    for selector in selectors:\n",
        "        sel_id = f\"C={selector.estimator.C}_thresh={selector.threshold}\"\n",
        "        print(f\"\\n>>> Feature set: {fs_name} | Selector: {sel_id}\")\n",
        "\n",
        "        results = perform_cross_validation(\n",
        "            X=X_feat,\n",
        "            y=features[\"y\"],\n",
        "            groups=features[\"groups\"],\n",
        "            estimator=estimator,\n",
        "            cats=cat_cols,\n",
        "            normalize=True,\n",
        "            select=[selector],\n",
        "            oversample=True,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        auc_scores = [r.metrics[\"AUC\"] for r in results]\n",
        "        avg_auc = np.mean(auc_scores)\n",
        "\n",
        "        print(f\"AUC Scores: {auc_scores}\")\n",
        "        print(f\"Mean AUC : {avg_auc:.4f}\")\n",
        "\n",
        "        pd.DataFrame({\n",
        "            \"AUC Scores\": auc_scores,\n",
        "            \"Mean AUC\": [avg_auc] * len(auc_scores)\n",
        "        }).to_csv(f\"assignment2_lasso{fs_name}__{sel_id}.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fJV9rxiKMwP",
        "outputId": "f926e0f7-7193-41ab-9e72-9dab9b8aa094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Feature set: feat_baseline | Selector: C=0.1_thresh=0.001\n",
            "Fold_0 - AUC: 0.6071 | Duration: 1.11s\n",
            "Fold_1 - AUC: 0.4882 | Duration: 1.00s\n",
            "Fold_2 - AUC: 0.5008 | Duration: 1.11s\n",
            "Fold_3 - AUC: 0.5548 | Duration: 1.05s\n",
            "Fold_4 - AUC: 0.5771 | Duration: 1.05s\n",
            "AUC Scores: [np.float64(0.6071306052438128), np.float64(0.4882139838183794), np.float64(0.5007865374062557), np.float64(0.5547628392016147), np.float64(0.5770954728821156)]\n",
            "Mean AUC : 0.5456\n",
            "\n",
            ">>> Feature set: feat_baseline | Selector: C=0.1_thresh=0.005\n",
            "Fold_0 - AUC: 0.6100 | Duration: 1.18s\n",
            "Fold_1 - AUC: 0.5241 | Duration: 1.02s\n",
            "Fold_2 - AUC: 0.5561 | Duration: 0.89s\n",
            "Fold_3 - AUC: 0.5611 | Duration: 1.07s\n",
            "Fold_4 - AUC: 0.5740 | Duration: 1.02s\n",
            "AUC Scores: [np.float64(0.6100465572163686), np.float64(0.5240671416495593), np.float64(0.556118529357966), np.float64(0.5610843238394259), np.float64(0.5740379074085932)]\n",
            "Mean AUC : 0.5651\n",
            "\n",
            ">>> Feature set: feat_baseline | Selector: C=0.1_thresh=mean\n",
            "Fold_0 - AUC: 0.6016 | Duration: 1.06s\n",
            "Fold_1 - AUC: 0.5250 | Duration: 0.96s\n",
            "Fold_2 - AUC: 0.5103 | Duration: 0.90s\n",
            "Fold_3 - AUC: 0.5376 | Duration: 1.04s\n",
            "Fold_4 - AUC: 0.5533 | Duration: 1.03s\n",
            "AUC Scores: [np.float64(0.6016417544719432), np.float64(0.5249728293684337), np.float64(0.5102707152002927), np.float64(0.5376345593182328), np.float64(0.5533233015303836)]\n",
            "Mean AUC : 0.5456\n",
            "\n",
            ">>> Feature set: feat_baseline | Selector: C=1.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.5993 | Duration: 6.14s\n",
            "Fold_1 - AUC: 0.5472 | Duration: 4.53s\n",
            "Fold_2 - AUC: 0.5590 | Duration: 4.98s\n",
            "Fold_3 - AUC: 0.5369 | Duration: 5.47s\n",
            "Fold_4 - AUC: 0.6000 | Duration: 4.09s\n",
            "AUC Scores: [np.float64(0.5992893898554276), np.float64(0.5471802922352373), np.float64(0.5590451801719407), np.float64(0.536863646557524), np.float64(0.6000352180316322)]\n",
            "Mean AUC : 0.5685\n",
            "\n",
            ">>> Feature set: feat_baseline | Selector: C=1.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.6111 | Duration: 6.09s\n",
            "Fold_1 - AUC: 0.5195 | Duration: 4.47s\n",
            "Fold_2 - AUC: 0.5945 | Duration: 4.89s\n",
            "Fold_3 - AUC: 0.5064 | Duration: 5.43s\n",
            "Fold_4 - AUC: 0.5734 | Duration: 4.22s\n",
            "AUC Scores: [np.float64(0.6111002205341828), np.float64(0.5195387030551866), np.float64(0.5945125297237974), np.float64(0.5063775510204082), np.float64(0.5733975795607351)]\n",
            "Mean AUC : 0.5610\n",
            "\n",
            ">>> Feature set: feat_baseline | Selector: C=1.0_thresh=mean\n",
            "Fold_0 - AUC: 0.5380 | Duration: 5.57s\n",
            "Fold_1 - AUC: 0.5267 | Duration: 3.92s\n",
            "Fold_2 - AUC: 0.5656 | Duration: 4.33s\n",
            "Fold_3 - AUC: 0.5229 | Duration: 4.93s\n",
            "Fold_4 - AUC: 0.6123 | Duration: 3.83s\n",
            "AUC Scores: [np.float64(0.5380298946336682), np.float64(0.5266996739524212), np.float64(0.5656392902871776), np.float64(0.5228891006952231), np.float64(0.6122814881219185)]\n",
            "Mean AUC : 0.5531\n",
            "\n",
            ">>> Feature set: feat_baseline | Selector: C=10.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.5747 | Duration: 42.89s\n",
            "Fold_1 - AUC: 0.5711 | Duration: 56.80s\n",
            "Fold_2 - AUC: 0.5601 | Duration: 33.36s\n",
            "Fold_3 - AUC: 0.5171 | Duration: 37.14s\n",
            "Fold_4 - AUC: 0.5681 | Duration: 41.77s\n",
            "AUC Scores: [np.float64(0.5746875765743689), np.float64(0.5711327134404057), np.float64(0.5600695079568319), np.float64(0.5171282798833818), np.float64(0.5681468912082986)]\n",
            "Mean AUC : 0.5582\n",
            "\n",
            ">>> Feature set: feat_baseline | Selector: C=10.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.5816 | Duration: 43.16s\n",
            "Fold_1 - AUC: 0.5076 | Duration: 57.09s\n",
            "Fold_2 - AUC: 0.5754 | Duration: 33.34s\n",
            "Fold_3 - AUC: 0.5120 | Duration: 36.91s\n",
            "Fold_4 - AUC: 0.6161 | Duration: 41.88s\n",
            "AUC Scores: [np.float64(0.5816221514334722), np.float64(0.5076198526747977), np.float64(0.5754344247301993), np.float64(0.5120262390670554), np.float64(0.6161074470128707)]\n",
            "Mean AUC : 0.5586\n",
            "\n",
            ">>> Feature set: feat_baseline | Selector: C=10.0_thresh=mean\n",
            "Fold_0 - AUC: 0.5656 | Duration: 41.88s\n",
            "Fold_1 - AUC: 0.4564 | Duration: 55.89s\n",
            "Fold_2 - AUC: 0.4623 | Duration: 32.28s\n",
            "Fold_3 - AUC: 0.5335 | Duration: 35.93s\n",
            "Fold_4 - AUC: 0.5384 | Duration: 40.68s\n",
            "AUC Scores: [np.float64(0.5656211712815487), np.float64(0.45640019321338), np.float64(0.46232851655386864), np.float64(0.5334856470060552), np.float64(0.5384196708714861)]\n",
            "Mean AUC : 0.5113\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm | Selector: C=0.1_thresh=0.001\n",
            "Fold_0 - AUC: 0.5999 | Duration: 2.89s\n",
            "Fold_1 - AUC: 0.5712 | Duration: 2.47s\n",
            "Fold_2 - AUC: 0.6131 | Duration: 3.04s\n",
            "Fold_3 - AUC: 0.5711 | Duration: 2.27s\n",
            "Fold_4 - AUC: 0.6244 | Duration: 2.96s\n",
            "AUC Scores: [np.float64(0.5999264886057338), np.float64(0.571229320130419), np.float64(0.6131059081763307), np.float64(0.571120206324288), np.float64(0.6244157008388295)]\n",
            "Mean AUC : 0.5960\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm | Selector: C=0.1_thresh=0.005\n",
            "Fold_0 - AUC: 0.6167 | Duration: 2.86s\n",
            "Fold_1 - AUC: 0.5632 | Duration: 2.38s\n",
            "Fold_2 - AUC: 0.6110 | Duration: 2.92s\n",
            "Fold_3 - AUC: 0.5787 | Duration: 2.32s\n",
            "Fold_4 - AUC: 0.6215 | Duration: 2.80s\n",
            "AUC Scores: [np.float64(0.6167360940945846), np.float64(0.5631988890230649), np.float64(0.6110023779037864), np.float64(0.5786611347835838), np.float64(0.621534225523468)]\n",
            "Mean AUC : 0.5982\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm | Selector: C=0.1_thresh=mean\n",
            "Fold_0 - AUC: 0.6025 | Duration: 2.73s\n",
            "Fold_1 - AUC: 0.5405 | Duration: 2.71s\n",
            "Fold_2 - AUC: 0.6329 | Duration: 2.80s\n",
            "Fold_3 - AUC: 0.5533 | Duration: 2.22s\n",
            "Fold_4 - AUC: 0.6238 | Duration: 2.80s\n",
            "AUC Scores: [np.float64(0.6024748836069591), np.float64(0.540538582296824), np.float64(0.6329065300896287), np.float64(0.5533471630410406), np.float64(0.6237673688928731)]\n",
            "Mean AUC : 0.5906\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm | Selector: C=1.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.6407 | Duration: 16.28s\n",
            "Fold_1 - AUC: 0.5252 | Duration: 11.24s\n",
            "Fold_2 - AUC: 0.5923 | Duration: 14.47s\n",
            "Fold_3 - AUC: 0.5399 | Duration: 12.58s\n",
            "Fold_4 - AUC: 0.6172 | Duration: 12.77s\n",
            "AUC Scores: [np.float64(0.6406763048272481), np.float64(0.5251901944209636), np.float64(0.5923175416133163), np.float64(0.5398631980264633), np.float64(0.6171719920599347)]\n",
            "Mean AUC : 0.5830\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm | Selector: C=1.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.5987 | Duration: 16.46s\n",
            "Fold_1 - AUC: 0.5871 | Duration: 11.65s\n",
            "Fold_2 - AUC: 0.6006 | Duration: 14.57s\n",
            "Fold_3 - AUC: 0.5321 | Duration: 12.71s\n",
            "Fold_4 - AUC: 0.6303 | Duration: 12.53s\n",
            "AUC Scores: [np.float64(0.5987258024993873), np.float64(0.5871150827194784), np.float64(0.6005578928114139), np.float64(0.5321400538237273), np.float64(0.6302586924505347)]\n",
            "Mean AUC : 0.5898\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm | Selector: C=1.0_thresh=mean\n",
            "Fold_0 - AUC: 0.6318 | Duration: 15.69s\n",
            "Fold_1 - AUC: 0.5515 | Duration: 10.96s\n",
            "Fold_2 - AUC: 0.6085 | Duration: 14.36s\n",
            "Fold_3 - AUC: 0.5439 | Duration: 11.93s\n",
            "Fold_4 - AUC: 0.5799 | Duration: 12.03s\n",
            "AUC Scores: [np.float64(0.6317569223229601), np.float64(0.5514792899408284), np.float64(0.6085055789281142), np.float64(0.5438579277864992), np.float64(0.5799049113145931)]\n",
            "Mean AUC : 0.5831\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm | Selector: C=10.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.6422 | Duration: 25.46s\n",
            "Fold_1 - AUC: 0.5684 | Duration: 17.48s\n",
            "Fold_2 - AUC: 0.6099 | Duration: 20.41s\n",
            "Fold_3 - AUC: 0.5775 | Duration: 17.97s\n",
            "Fold_4 - AUC: 0.5846 | Duration: 19.48s\n",
            "AUC Scores: [np.float64(0.6421710365106591), np.float64(0.5683733848569013), np.float64(0.6098500091457838), np.float64(0.5774977573446961), np.float64(0.5845873087020554)]\n",
            "Mean AUC : 0.5965\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm | Selector: C=10.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.6401 | Duration: 24.68s\n",
            "Fold_1 - AUC: 0.5773 | Duration: 16.99s\n",
            "Fold_2 - AUC: 0.6036 | Duration: 20.03s\n",
            "Fold_3 - AUC: 0.5434 | Duration: 18.05s\n",
            "Fold_4 - AUC: 0.6230 | Duration: 19.55s\n",
            "AUC Scores: [np.float64(0.640112717471208), np.float64(0.5772672382562493), np.float64(0.6036034388147065), np.float64(0.5433533303431263), np.float64(0.6229669590830504)]\n",
            "Mean AUC : 0.5975\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm | Selector: C=10.0_thresh=mean\n",
            "Fold_0 - AUC: 0.5402 | Duration: 24.11s\n",
            "Fold_1 - AUC: 0.5557 | Duration: 16.30s\n",
            "Fold_2 - AUC: 0.6322 | Duration: 19.65s\n",
            "Fold_3 - AUC: 0.5596 | Duration: 17.13s\n",
            "Fold_4 - AUC: 0.6341 | Duration: 18.27s\n",
            "AUC Scores: [np.float64(0.5401617250673855), np.float64(0.5556997947107837), np.float64(0.6322388878726907), np.float64(0.5595985647006055), np.float64(0.6341006595376834)]\n",
            "Mean AUC : 0.5844\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday | Selector: C=0.1_thresh=0.001\n",
            "Fold_0 - AUC: 0.5827 | Duration: 3.53s\n",
            "Fold_1 - AUC: 0.5830 | Duration: 2.87s\n",
            "Fold_2 - AUC: 0.5626 | Duration: 3.22s\n",
            "Fold_3 - AUC: 0.5226 | Duration: 3.10s\n",
            "Fold_4 - AUC: 0.6455 | Duration: 3.19s\n",
            "AUC Scores: [np.float64(0.5827003185493752), np.float64(0.5830213742301654), np.float64(0.5625663069325041), np.float64(0.5225527023996412), np.float64(0.645514503425754)]\n",
            "Mean AUC : 0.5793\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday | Selector: C=0.1_thresh=0.005\n",
            "Fold_0 - AUC: 0.5626 | Duration: 3.47s\n",
            "Fold_1 - AUC: 0.6012 | Duration: 2.73s\n",
            "Fold_2 - AUC: 0.6003 | Duration: 3.31s\n",
            "Fold_3 - AUC: 0.5083 | Duration: 2.89s\n",
            "Fold_4 - AUC: 0.6296 | Duration: 3.24s\n",
            "AUC Scores: [np.float64(0.5626317079147267), np.float64(0.6011592802801594), np.float64(0.6003475397841594), np.float64(0.508339874411303), np.float64(0.629634372798873)]\n",
            "Mean AUC : 0.5804\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday | Selector: C=0.1_thresh=mean\n",
            "Fold_0 - AUC: 0.6107 | Duration: 3.49s\n",
            "Fold_1 - AUC: 0.5221 | Duration: 2.77s\n",
            "Fold_2 - AUC: 0.5834 | Duration: 3.02s\n",
            "Fold_3 - AUC: 0.5368 | Duration: 2.89s\n",
            "Fold_4 - AUC: 0.6094 | Duration: 3.23s\n",
            "AUC Scores: [np.float64(0.6107081597647634), np.float64(0.5221289699311678), np.float64(0.583372965063106), np.float64(0.5368356133662257), np.float64(0.6094400332970481)]\n",
            "Mean AUC : 0.5725\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday | Selector: C=1.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.6289 | Duration: 12.76s\n",
            "Fold_1 - AUC: 0.5976 | Duration: 9.38s\n",
            "Fold_2 - AUC: 0.6036 | Duration: 12.12s\n",
            "Fold_3 - AUC: 0.5346 | Duration: 11.10s\n",
            "Fold_4 - AUC: 0.5785 | Duration: 11.37s\n",
            "AUC Scores: [np.float64(0.6288899779465817), np.float64(0.5976089844221713), np.float64(0.6036125845985001), np.float64(0.5346209912536444), np.float64(0.5784961900493052)]\n",
            "Mean AUC : 0.5886\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday | Selector: C=1.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.6280 | Duration: 12.98s\n",
            "Fold_1 - AUC: 0.6216 | Duration: 9.50s\n",
            "Fold_2 - AUC: 0.5959 | Duration: 12.26s\n",
            "Fold_3 - AUC: 0.5462 | Duration: 11.20s\n",
            "Fold_4 - AUC: 0.6040 | Duration: 10.64s\n",
            "AUC Scores: [np.float64(0.6280323450134772), np.float64(0.6216278227267238), np.float64(0.595911834644229), np.float64(0.5461986992599237), np.float64(0.6039572260997632)]\n",
            "Mean AUC : 0.5991\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday | Selector: C=1.0_thresh=mean\n",
            "Fold_0 - AUC: 0.6235 | Duration: 12.35s\n",
            "Fold_1 - AUC: 0.5641 | Duration: 9.25s\n",
            "Fold_2 - AUC: 0.5652 | Duration: 11.85s\n",
            "Fold_3 - AUC: 0.5183 | Duration: 10.70s\n",
            "Fold_4 - AUC: 0.5831 | Duration: 10.61s\n",
            "AUC Scores: [np.float64(0.6235481499632443), np.float64(0.5641468421688202), np.float64(0.5652185842326688), np.float64(0.5183196905135681), np.float64(0.5830985464557853)]\n",
            "Mean AUC : 0.5709\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday | Selector: C=10.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.6388 | Duration: 44.91s\n",
            "Fold_1 - AUC: 0.5474 | Duration: 16.16s\n",
            "Fold_2 - AUC: 0.5772 | Duration: 16.98s\n",
            "Fold_3 - AUC: 0.5690 | Duration: 13.66s\n",
            "Fold_4 - AUC: 0.5975 | Duration: 17.05s\n",
            "AUC Scores: [np.float64(0.6388140161725067), np.float64(0.5474338847965221), np.float64(0.5772087067861715), np.float64(0.5689616505943037), np.float64(0.5975379394249856)]\n",
            "Mean AUC : 0.5860\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday | Selector: C=10.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.5867 | Duration: 45.17s\n",
            "Fold_1 - AUC: 0.5588 | Duration: 16.01s\n",
            "Fold_2 - AUC: 0.6085 | Duration: 17.04s\n",
            "Fold_3 - AUC: 0.5718 | Duration: 13.85s\n",
            "Fold_4 - AUC: 0.6421 | Duration: 16.96s\n",
            "AUC Scores: [np.float64(0.5867189414359226), np.float64(0.5587791329549572), np.float64(0.6085147247119078), np.float64(0.5718350527023996), np.float64(0.6421287699302043)]\n",
            "Mean AUC : 0.5936\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday | Selector: C=10.0_thresh=mean\n",
            "Fold_0 - AUC: 0.5824 | Duration: 44.90s\n",
            "Fold_1 - AUC: 0.5713 | Duration: 15.75s\n",
            "Fold_2 - AUC: 0.5721 | Duration: 16.69s\n",
            "Fold_3 - AUC: 0.5209 | Duration: 13.76s\n",
            "Fold_4 - AUC: 0.5913 | Duration: 16.47s\n",
            "AUC Scores: [np.float64(0.5824062729723107), np.float64(0.5713017751479289), np.float64(0.5721236509968904), np.float64(0.5209267773043283), np.float64(0.5913347633988602)]\n",
            "Mean AUC : 0.5676\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday-no_immediate_past | Selector: C=0.1_thresh=0.001\n",
            "Fold_0 - AUC: 0.5618 | Duration: 3.18s\n",
            "Fold_1 - AUC: 0.5734 | Duration: 2.65s\n",
            "Fold_2 - AUC: 0.5880 | Duration: 2.92s\n",
            "Fold_3 - AUC: 0.5167 | Duration: 2.96s\n",
            "Fold_4 - AUC: 0.5963 | Duration: 3.18s\n",
            "AUC Scores: [np.float64(0.5618475863758883), np.float64(0.5734210844100954), np.float64(0.5879824400951161), np.float64(0.5166937654182552), np.float64(0.5963213165140552)]\n",
            "Mean AUC : 0.5673\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday-no_immediate_past | Selector: C=0.1_thresh=0.005\n",
            "Fold_0 - AUC: 0.5659 | Duration: 3.36s\n",
            "Fold_1 - AUC: 0.5438 | Duration: 2.45s\n",
            "Fold_2 - AUC: 0.5525 | Duration: 2.84s\n",
            "Fold_3 - AUC: 0.5655 | Duration: 2.91s\n",
            "Fold_4 - AUC: 0.6508 | Duration: 3.08s\n",
            "AUC Scores: [np.float64(0.5658907130605244), np.float64(0.5438413235116532), np.float64(0.5525425278946405), np.float64(0.5655486095537116), np.float64(0.6508452327591727)]\n",
            "Mean AUC : 0.5757\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday-no_immediate_past | Selector: C=0.1_thresh=mean\n",
            "Fold_0 - AUC: 0.6023 | Duration: 3.36s\n",
            "Fold_1 - AUC: 0.5959 | Duration: 2.45s\n",
            "Fold_2 - AUC: 0.5781 | Duration: 2.82s\n",
            "Fold_3 - AUC: 0.5601 | Duration: 2.87s\n",
            "Fold_4 - AUC: 0.6100 | Duration: 3.11s\n",
            "AUC Scores: [np.float64(0.6022666013232051), np.float64(0.5959062915106871), np.float64(0.5780592646789829), np.float64(0.5600961538461539), np.float64(0.6100083242620222)]\n",
            "Mean AUC : 0.5893\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday-no_immediate_past | Selector: C=1.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.7046 | Duration: 14.53s\n",
            "Fold_1 - AUC: 0.6169 | Duration: 11.46s\n",
            "Fold_2 - AUC: 0.5989 | Duration: 12.93s\n",
            "Fold_3 - AUC: 0.5246 | Duration: 11.87s\n",
            "Fold_4 - AUC: 0.6116 | Duration: 12.49s\n",
            "AUC Scores: [np.float64(0.7046067140406763), np.float64(0.6168940949160729), np.float64(0.5989207975123468), np.float64(0.5245570755774838), np.float64(0.6116411602740603)]\n",
            "Mean AUC : 0.6113\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday-no_immediate_past | Selector: C=1.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.6044 | Duration: 14.62s\n",
            "Fold_1 - AUC: 0.5934 | Duration: 11.99s\n",
            "Fold_2 - AUC: 0.5670 | Duration: 12.93s\n",
            "Fold_3 - AUC: 0.5653 | Duration: 11.83s\n",
            "Fold_4 - AUC: 0.6356 | Duration: 12.18s\n",
            "AUC Scores: [np.float64(0.6043616760597893), np.float64(0.5934367829972226), np.float64(0.5670385952076092), np.float64(0.5653033191298498), np.float64(0.6356294422744445)]\n",
            "Mean AUC : 0.5932\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday-no_immediate_past | Selector: C=1.0_thresh=mean\n",
            "Fold_0 - AUC: 0.6706 | Duration: 14.08s\n",
            "Fold_1 - AUC: 0.5764 | Duration: 11.48s\n",
            "Fold_2 - AUC: 0.5895 | Duration: 12.65s\n",
            "Fold_3 - AUC: 0.5548 | Duration: 11.39s\n",
            "Fold_4 - AUC: 0.6071 | Duration: 12.20s\n",
            "AUC Scores: [np.float64(0.6706444498897329), np.float64(0.5764400434730105), np.float64(0.5894914944210718), np.float64(0.554776855797264), np.float64(0.6070788243580714)]\n",
            "Mean AUC : 0.5997\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday-no_immediate_past | Selector: C=10.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.5493 | Duration: 27.54s\n",
            "Fold_1 - AUC: 0.5390 | Duration: 19.89s\n",
            "Fold_2 - AUC: 0.6339 | Duration: 24.74s\n",
            "Fold_3 - AUC: 0.5338 | Duration: 22.84s\n",
            "Fold_4 - AUC: 0.6094 | Duration: 25.08s\n",
            "AUC Scores: [np.float64(0.5492771379563832), np.float64(0.5389989131747374), np.float64(0.6338851289555514), np.float64(0.5337799955146895), np.float64(0.6094160210027534)]\n",
            "Mean AUC : 0.5731\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday-no_immediate_past | Selector: C=10.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.5700 | Duration: 28.44s\n",
            "Fold_1 - AUC: 0.6210 | Duration: 19.55s\n",
            "Fold_2 - AUC: 0.5495 | Duration: 24.55s\n",
            "Fold_3 - AUC: 0.5316 | Duration: 22.85s\n",
            "Fold_4 - AUC: 0.6183 | Duration: 24.79s\n",
            "AUC Scores: [np.float64(0.5700073511394266), np.float64(0.6209515758966309), np.float64(0.5494878361075544), np.float64(0.5315513568064589), np.float64(0.6182765575974899)]\n",
            "Mean AUC : 0.5781\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday-no_immediate_past | Selector: C=10.0_thresh=mean\n",
            "Fold_0 - AUC: 0.5883 | Duration: 27.10s\n",
            "Fold_1 - AUC: 0.5661 | Duration: 19.76s\n",
            "Fold_2 - AUC: 0.6229 | Duration: 24.77s\n",
            "Fold_3 - AUC: 0.5801 | Duration: 22.57s\n",
            "Fold_4 - AUC: 0.5987 | Duration: 24.68s\n",
            "AUC Scores: [np.float64(0.5882749326145553), np.float64(0.5660548242965825), np.float64(0.6228827510517652), np.float64(0.5801188607311056), np.float64(0.5987065377473266)]\n",
            "Mean AUC : 0.5912\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday+immediate_past_esm | Selector: C=0.1_thresh=0.001\n",
            "Fold_0 - AUC: 0.5827 | Duration: 3.58s\n",
            "Fold_1 - AUC: 0.5830 | Duration: 2.84s\n",
            "Fold_2 - AUC: 0.5626 | Duration: 3.15s\n",
            "Fold_3 - AUC: 0.5226 | Duration: 3.11s\n",
            "Fold_4 - AUC: 0.6455 | Duration: 3.22s\n",
            "AUC Scores: [np.float64(0.5827003185493752), np.float64(0.5830213742301654), np.float64(0.5625663069325041), np.float64(0.5225527023996412), np.float64(0.645514503425754)]\n",
            "Mean AUC : 0.5793\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday+immediate_past_esm | Selector: C=0.1_thresh=0.005\n",
            "Fold_0 - AUC: 0.5626 | Duration: 3.44s\n",
            "Fold_1 - AUC: 0.6012 | Duration: 2.68s\n",
            "Fold_2 - AUC: 0.6003 | Duration: 3.26s\n",
            "Fold_3 - AUC: 0.5083 | Duration: 2.87s\n",
            "Fold_4 - AUC: 0.6296 | Duration: 3.24s\n",
            "AUC Scores: [np.float64(0.5626317079147267), np.float64(0.6011592802801594), np.float64(0.6003475397841594), np.float64(0.508339874411303), np.float64(0.629634372798873)]\n",
            "Mean AUC : 0.5804\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday+immediate_past_esm | Selector: C=0.1_thresh=mean\n",
            "Fold_0 - AUC: 0.6107 | Duration: 3.48s\n",
            "Fold_1 - AUC: 0.5221 | Duration: 2.79s\n",
            "Fold_2 - AUC: 0.5834 | Duration: 3.08s\n",
            "Fold_3 - AUC: 0.5368 | Duration: 2.89s\n",
            "Fold_4 - AUC: 0.6094 | Duration: 3.25s\n",
            "AUC Scores: [np.float64(0.6107081597647634), np.float64(0.5221289699311678), np.float64(0.583372965063106), np.float64(0.5368356133662257), np.float64(0.6094400332970481)]\n",
            "Mean AUC : 0.5725\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday+immediate_past_esm | Selector: C=1.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.6289 | Duration: 12.89s\n",
            "Fold_1 - AUC: 0.5976 | Duration: 9.62s\n",
            "Fold_2 - AUC: 0.6036 | Duration: 12.26s\n",
            "Fold_3 - AUC: 0.5346 | Duration: 11.41s\n",
            "Fold_4 - AUC: 0.5785 | Duration: 11.01s\n",
            "AUC Scores: [np.float64(0.6288899779465817), np.float64(0.5976089844221713), np.float64(0.6036125845985001), np.float64(0.5346209912536444), np.float64(0.5784961900493052)]\n",
            "Mean AUC : 0.5886\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday+immediate_past_esm | Selector: C=1.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.6280 | Duration: 13.03s\n",
            "Fold_1 - AUC: 0.6216 | Duration: 9.44s\n",
            "Fold_2 - AUC: 0.5959 | Duration: 11.97s\n",
            "Fold_3 - AUC: 0.5462 | Duration: 11.04s\n",
            "Fold_4 - AUC: 0.6040 | Duration: 10.66s\n",
            "AUC Scores: [np.float64(0.6280323450134772), np.float64(0.6216278227267238), np.float64(0.595911834644229), np.float64(0.5461986992599237), np.float64(0.6039572260997632)]\n",
            "Mean AUC : 0.5991\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday+immediate_past_esm | Selector: C=1.0_thresh=mean\n",
            "Fold_0 - AUC: 0.6235 | Duration: 12.42s\n",
            "Fold_1 - AUC: 0.5641 | Duration: 9.14s\n",
            "Fold_2 - AUC: 0.5652 | Duration: 11.79s\n",
            "Fold_3 - AUC: 0.5183 | Duration: 10.62s\n",
            "Fold_4 - AUC: 0.5831 | Duration: 10.60s\n",
            "AUC Scores: [np.float64(0.6235481499632443), np.float64(0.5641468421688202), np.float64(0.5652185842326688), np.float64(0.5183196905135681), np.float64(0.5830985464557853)]\n",
            "Mean AUC : 0.5709\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday+immediate_past_esm | Selector: C=10.0_thresh=0.001\n",
            "Fold_0 - AUC: 0.6388 | Duration: 44.90s\n",
            "Fold_1 - AUC: 0.5474 | Duration: 16.13s\n",
            "Fold_2 - AUC: 0.5772 | Duration: 17.05s\n",
            "Fold_3 - AUC: 0.5690 | Duration: 13.74s\n",
            "Fold_4 - AUC: 0.5975 | Duration: 17.00s\n",
            "AUC Scores: [np.float64(0.6388140161725067), np.float64(0.5474338847965221), np.float64(0.5772087067861715), np.float64(0.5689616505943037), np.float64(0.5975379394249856)]\n",
            "Mean AUC : 0.5860\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday+immediate_past_esm | Selector: C=10.0_thresh=0.005\n",
            "Fold_0 - AUC: 0.5867 | Duration: 45.17s\n",
            "Fold_1 - AUC: 0.5588 | Duration: 15.89s\n",
            "Fold_2 - AUC: 0.6085 | Duration: 17.11s\n",
            "Fold_3 - AUC: 0.5718 | Duration: 13.86s\n",
            "Fold_4 - AUC: 0.6421 | Duration: 17.02s\n",
            "AUC Scores: [np.float64(0.5867189414359226), np.float64(0.5587791329549572), np.float64(0.6085147247119078), np.float64(0.5718350527023996), np.float64(0.6421287699302043)]\n",
            "Mean AUC : 0.5936\n",
            "\n",
            ">>> Feature set: baseline+today+current_esm+yesterday+immediate_past_esm | Selector: C=10.0_thresh=mean\n",
            "Fold_0 - AUC: 0.5824 | Duration: 44.51s\n",
            "Fold_1 - AUC: 0.5713 | Duration: 15.70s\n",
            "Fold_2 - AUC: 0.5721 | Duration: 16.16s\n",
            "Fold_3 - AUC: 0.5209 | Duration: 13.62s\n",
            "Fold_4 - AUC: 0.5913 | Duration: 16.33s\n",
            "AUC Scores: [np.float64(0.5824062729723107), np.float64(0.5713017751479289), np.float64(0.5721236509968904), np.float64(0.5209267773043283), np.float64(0.5913347633988602)]\n",
            "Mean AUC : 0.5676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso: Discussion\n",
        "\n",
        "The highest AUC (0.6113) was achieved when immediate past ESM features were excluded, suggesting potential overfitting from including temporally close data. In general, lower thresholds like 0.001 led to overly sparse models and underperformance, while very high regularization (C=10) increased computation time without consistent gains. Therefore, we again restate the importance of carefully balancing regularization strength and feature inclusion for optimal performance."
      ],
      "metadata": {
        "id": "boN7ky8gW4Qq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SHAP-Based Feature Selection\n",
        "\n",
        "We apply SHAP (SHapley Additive exPlanations) to understand feature importance and guide selection.\n",
        "\n",
        "Two helper functions are used:\n",
        "\n",
        "- `get_shap_feature_ranking(X, y, max_samples=1000, random_state=42)`  \n",
        "   Computes SHAP values for a trained model using a subset of samples for efficiency.  \n",
        "   Returns features ranked by their mean absolute SHAP value.\n",
        "\n",
        "- `select_top_features_by_shap(X, y, top_n: int = 20)`  \n",
        "   Selects the top-N most influential features based on SHAP ranking.  \n",
        "   Returns a reduced feature matrix for model training or evaluation.\n",
        "\n",
        "Also, we use EvXGBooster as the model for SHAP-based exploration.\n",
        "\n",
        "SHAP is model-agnostic and especially effective for interpreting tree-based models like XGBoost. It helps us select features that contribute most to model decisions."
      ],
      "metadata": {
        "id": "hnUgtGfyKwKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "def get_shap_feature_ranking(X, y, max_samples=1000, random_state=42):\n",
        "    \"\"\"\n",
        "    Compute SHAP-based feature importance scores using a fitted XGBoost model.\n",
        "\n",
        "    Parameters:\n",
        "        X (pd.DataFrame): Input feature matrix.\n",
        "        y (array-like): Target variable.\n",
        "        max_samples (int): Maximum number of samples to use for computing SHAP values (for efficiency).\n",
        "        random_state (int): Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Features ranked by mean absolute SHAP values (descending order).\n",
        "    \"\"\"\n",
        "    model = EvXGBClassifier(eval_size=None, random_state=random_state)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    explainer = shap.Explainer(model.model)\n",
        "    sample_X = X.iloc[:max_samples]\n",
        "    shap_values = explainer(sample_X).values\n",
        "    mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "    return pd.Series(mean_abs_shap, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "def select_top_features_by_shap(X, y, top_n: int = 20) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Select the top-N most important features based on SHAP values.\n",
        "\n",
        "    Parameters:\n",
        "        X (pd.DataFrame): Input feature matrix.\n",
        "        y (array-like): Target variable.\n",
        "        top_n (int): Number of top features to select based on SHAP importance.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Reduced feature matrix containing only the top-N SHAP-ranked features.\n",
        "    \"\"\"\n",
        "    ranking = get_shap_feature_ranking(X, y)\n",
        "    top_features = ranking.head(top_n).index\n",
        "    return X[top_features]"
      ],
      "metadata": {
        "id": "5cAqTieTMGKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All features were used to avoid pre filtering features as SHAP captures global and local importance. However we can add any other feature combinations to test"
      ],
      "metadata": {
        "id": "3coz7wLLO9bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_sets = {\n",
        "    # \"baseline\": features[\"feat_baseline\"], #We can also consider the original baseline just for comparison\n",
        "    \"all_features\": features[\"X_cleaned\"],\n",
        "}\n",
        "\n",
        "\n",
        "# Define classifier (Baseline)\n",
        "estimator = EvXGBClassifier(\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic',\n",
        "    verbosity=0,\n",
        "    learning_rate=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "Gh8jtbzN0SdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Leakage Caution\n",
        "\n",
        "SHAP values are powerful for interpreting model predictions, but computing them on the entire dataset before splitting risks data leakagewhere information from test samples inadvertently influences feature selection.\n",
        "\n",
        "* **Cross-Validation Alignment**: To prevent this, we mimicked the exact splits used during evaluation by running `StratifiedGroupKFold` with `n_splits=5` and a fixed `random_state=42`. This ensured consistent partitioning across both SHAP computation and final model evaluation.\n",
        "\n",
        "* **Train-Only SHAP Extraction**: For SHAP feature ranking, we merged the training indices from all five folds (excluding test data entirely). This merged training set was used to fit a model and compute SHAP values.\n",
        "\n",
        "* **Top-N Feature Selection**: From the SHAP values, we selected the top-N most important features (based on mean absolute SHAP value) and used these same features for all test folds.\n",
        "\n",
        "* **Leakage-Free Evaluation**: By isolating SHAP computation to training data only and keeping the test data untouched during selection, we ensured that our feature selection process remained leakage-free and evaluation results remained reliable.\n",
        "\n",
        "We evaluate SHAP-based feature selection by progressively keeping the top-K features from all 5,589."
      ],
      "metadata": {
        "id": "QgbxEaTxPKbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N_list = [10, 20, 30, 40, 45, 50, 60, 65, 70, 80, 90, 100, 200, 300, 500]\n",
        "\n",
        "#directory for saving values\n",
        "result_dir = \"assignment2_shap/shap\"\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "# Loop over each TOP_N value and feature set\n",
        "\n",
        "for TOP_N in TOP_N_list:\n",
        "    for fs_name, X_base in feature_sets.items():\n",
        "\n",
        "        # Skip if not enough features\n",
        "        if X_base.shape[1] < TOP_N:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n Feature Set: {fs_name}\")\n",
        "\n",
        "        # ----------------------------\n",
        "        # Get all training data across folds using StratifiedGroupKFold\n",
        "        # This simulates what training data looks like across CV splits,\n",
        "        # allowing SHAP feature selection without touching any test fold.\n",
        "        # ----------------------------\n",
        "        cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        train_indices = []\n",
        "\n",
        "        for train_idx, _ in cv.split(X_base, features[\"y\"], features[\"groups\"]):\n",
        "            train_indices.extend(train_idx)\n",
        "\n",
        "        # Remove duplicates from aggregated training indices\n",
        "        unique_train_indices = sorted(set(train_indices))\n",
        "\n",
        "        # Extract merged training data for SHAP feature selection\n",
        "        X_train_merged = X_base.iloc[unique_train_indices]\n",
        "        y_train_merged = features[\"y\"][unique_train_indices]\n",
        "\n",
        "        # Run SHAP feature selection on merged training data\n",
        "        X_selected = select_top_features_by_shap(X_train_merged, y_train_merged, top_n=TOP_N)\n",
        "        selected_features = X_selected.columns.tolist()\n",
        "        selected_features.sort()\n",
        "\n",
        "        print(f\"Selected {len(selected_features)} features from merged training data.\")\n",
        "        print(f\"SHAP feature selection for TOP_N = {TOP_N}\")\n",
        "\n",
        "        # Apply selected features to full dataset for evaluation\n",
        "        X_shap = X_base[selected_features]\n",
        "\n",
        "        # Identify categorical columns\n",
        "        cat_cols = X_shap.columns[X_shap.dtypes == bool]\n",
        "\n",
        "        # Run model evaluation using existing CV pipeline\n",
        "        results = perform_cross_validation(\n",
        "            X_shap, features[\"y\"], features[\"groups\"],\n",
        "            estimator=estimator,\n",
        "            cats=cat_cols,\n",
        "            normalize=True,\n",
        "            select=None,  # SHAP already selects\n",
        "            oversample=True,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Save evaluation results\n",
        "        auc_scores = [r.metrics[\"AUC\"] for r in results]\n",
        "        avg_auc = np.mean(auc_scores)\n",
        "\n",
        "        print(f\"\\n AUC Scores: {auc_scores}\")\n",
        "        print(f\" Mean AUC: {avg_auc:.4f}\")\n",
        "\n",
        "        pd.DataFrame({\n",
        "            \"AUC Scores\": auc_scores,\n",
        "            \"Mean AUC\": [avg_auc] * len(auc_scores)\n",
        "        }).to_csv(os.path.join(result_dir, f\"{fs_name}__top{TOP_N}.csv\"), index=False)\n",
        "\n",
        "        # Save selected feature names\n",
        "        pd.Series(selected_features).to_csv(\n",
        "            os.path.join(result_dir, f\"{fs_name}__top{TOP_N}_features.csv\"),\n",
        "            index=False, header=False\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjc7J0mlOGXH",
        "outputId": "b5087213-4715-40cc-802a-c1c7572a7273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature Set: all_features\n",
            "Selected 10 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 10\n",
            "Fold_0 - AUC: 0.6384 | Duration: 0.11s\n",
            "Fold_1 - AUC: 0.6108 | Duration: 0.10s\n",
            "Fold_2 - AUC: 0.5858 | Duration: 0.10s\n",
            "Fold_3 - AUC: 0.6176 | Duration: 0.12s\n",
            "Fold_4 - AUC: 0.6068 | Duration: 0.15s\n",
            "\n",
            " AUC Scores: [np.float64(0.6384097035040431), np.float64(0.6107595701002295), np.float64(0.5858423266873971), np.float64(0.6176342789863198), np.float64(0.6067906768265352)]\n",
            " Mean AUC: 0.6119\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 20 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 20\n",
            "Fold_0 - AUC: 0.6592 | Duration: 0.15s\n",
            "Fold_1 - AUC: 0.6068 | Duration: 0.16s\n",
            "Fold_2 - AUC: 0.6266 | Duration: 0.13s\n",
            "Fold_3 - AUC: 0.6264 | Duration: 0.16s\n",
            "Fold_4 - AUC: 0.6397 | Duration: 0.17s\n",
            "\n",
            " AUC Scores: [np.float64(0.6592011761823083), np.float64(0.6068469991546914), np.float64(0.6265502103530274), np.float64(0.6263596097779771), np.float64(0.6396555036178524)]\n",
            " Mean AUC: 0.6317\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 30 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 30\n",
            "Fold_0 - AUC: 0.6743 | Duration: 0.16s\n",
            "Fold_1 - AUC: 0.6347 | Duration: 0.17s\n",
            "Fold_2 - AUC: 0.6566 | Duration: 0.14s\n",
            "Fold_3 - AUC: 0.6398 | Duration: 0.14s\n",
            "Fold_4 - AUC: 0.6003 | Duration: 0.16s\n",
            "\n",
            " AUC Scores: [np.float64(0.6743200196030384), np.float64(0.6346576500422654), np.float64(0.6566306932504117), np.float64(0.6397875084099574), np.float64(0.6002833450726772)]\n",
            " Mean AUC: 0.6411\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 40 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 40\n",
            "Fold_0 - AUC: 0.7129 | Duration: 0.22s\n",
            "Fold_1 - AUC: 0.6256 | Duration: 0.20s\n",
            "Fold_2 - AUC: 0.6597 | Duration: 0.17s\n",
            "Fold_3 - AUC: 0.6232 | Duration: 0.16s\n",
            "Fold_4 - AUC: 0.6515 | Duration: 0.16s\n",
            "\n",
            " AUC Scores: [np.float64(0.7128644939965696), np.float64(0.625564545344765), np.float64(0.6596670934699103), np.float64(0.6231918591612469), np.float64(0.6514535442146379)]\n",
            " Mean AUC: 0.6545\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 45 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 45\n",
            "Fold_0 - AUC: 0.6933 | Duration: 0.16s\n",
            "Fold_1 - AUC: 0.6239 | Duration: 0.19s\n",
            "Fold_2 - AUC: 0.6711 | Duration: 0.17s\n",
            "Fold_3 - AUC: 0.6311 | Duration: 0.20s\n",
            "Fold_4 - AUC: 0.6644 | Duration: 0.20s\n",
            "\n",
            " AUC Scores: [np.float64(0.6932614555256065), np.float64(0.6238860041057843), np.float64(0.6711176147795865), np.float64(0.6310972191074232), np.float64(0.6644041749375681)]\n",
            " Mean AUC: 0.6568\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 50 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 50\n",
            "Fold_0 - AUC: 0.7052 | Duration: 0.18s\n",
            "Fold_1 - AUC: 0.6212 | Duration: 0.18s\n",
            "Fold_2 - AUC: 0.6807 | Duration: 0.21s\n",
            "Fold_3 - AUC: 0.6052 | Duration: 0.18s\n",
            "Fold_4 - AUC: 0.6540 | Duration: 0.21s\n",
            "\n",
            " AUC Scores: [np.float64(0.7051703013967165), np.float64(0.6212172442941675), np.float64(0.6806566672763856), np.float64(0.6052085669432608), np.float64(0.6540148556060703)]\n",
            " Mean AUC: 0.6533\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 60 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 60\n",
            "Fold_0 - AUC: 0.6778 | Duration: 0.23s\n",
            "Fold_1 - AUC: 0.6421 | Duration: 0.20s\n",
            "Fold_2 - AUC: 0.6833 | Duration: 0.19s\n",
            "Fold_3 - AUC: 0.6258 | Duration: 0.20s\n",
            "Fold_4 - AUC: 0.6756 | Duration: 0.19s\n",
            "\n",
            " AUC Scores: [np.float64(0.6778485665278118), np.float64(0.6420963651732882), np.float64(0.6832723614413755), np.float64(0.6258129625476564), np.float64(0.6755618876864955)]\n",
            " Mean AUC: 0.6609\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 65 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 65\n",
            "Fold_0 - AUC: 0.7079 | Duration: 0.19s\n",
            "Fold_1 - AUC: 0.6244 | Duration: 0.22s\n",
            "Fold_2 - AUC: 0.6605 | Duration: 0.23s\n",
            "Fold_3 - AUC: 0.6459 | Duration: 0.23s\n",
            "Fold_4 - AUC: 0.6317 | Duration: 0.18s\n",
            "\n",
            " AUC Scores: [np.float64(0.7078657191864739), np.float64(0.6244414925733607), np.float64(0.660453630876166), np.float64(0.6458987441130298), np.float64(0.6316994301082155)]\n",
            " Mean AUC: 0.6541\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 70 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 70\n",
            "Fold_0 - AUC: 0.6908 | Duration: 0.21s\n",
            "Fold_1 - AUC: 0.6317 | Duration: 0.27s\n",
            "Fold_2 - AUC: 0.6674 | Duration: 0.19s\n",
            "Fold_3 - AUC: 0.6310 | Duration: 0.21s\n",
            "Fold_4 - AUC: 0.6564 | Duration: 0.21s\n",
            "\n",
            " AUC Scores: [np.float64(0.6907620681205586), np.float64(0.6317111459968603), np.float64(0.6673952807755625), np.float64(0.6310411527248262), np.float64(0.6564481014279311)]\n",
            " Mean AUC: 0.6555\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 80 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 80\n",
            "Fold_0 - AUC: 0.6010 | Duration: 0.20s\n",
            "Fold_1 - AUC: 0.6214 | Duration: 0.24s\n",
            "Fold_2 - AUC: 0.6645 | Duration: 0.19s\n",
            "Fold_3 - AUC: 0.6018 | Duration: 0.24s\n",
            "Fold_4 - AUC: 0.6174 | Duration: 0.25s\n",
            "\n",
            " AUC Scores: [np.float64(0.6009556481254594), np.float64(0.6214346093466974), np.float64(0.6644869215291751), np.float64(0.6018165507961426), np.float64(0.6173560863161938)]\n",
            " Mean AUC: 0.6212\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 90 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 90\n",
            "Fold_0 - AUC: 0.6400 | Duration: 0.21s\n",
            "Fold_1 - AUC: 0.6131 | Duration: 0.23s\n",
            "Fold_2 - AUC: 0.6433 | Duration: 0.20s\n",
            "Fold_3 - AUC: 0.6104 | Duration: 0.17s\n",
            "Fold_4 - AUC: 0.6588 | Duration: 0.24s\n",
            "\n",
            " AUC Scores: [np.float64(0.6399901984807644), np.float64(0.6131264340055549), np.float64(0.6433052862630327), np.float64(0.6103666741421843), np.float64(0.6588173144650061)]\n",
            " Mean AUC: 0.6331\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 100 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 100\n",
            "Fold_0 - AUC: 0.6442 | Duration: 0.22s\n",
            "Fold_1 - AUC: 0.6503 | Duration: 0.23s\n",
            "Fold_2 - AUC: 0.6813 | Duration: 0.20s\n",
            "Fold_3 - AUC: 0.5992 | Duration: 0.21s\n",
            "Fold_4 - AUC: 0.6724 | Duration: 0.20s\n",
            "\n",
            " AUC Scores: [np.float64(0.6441558441558443), np.float64(0.6503320854969207), np.float64(0.6812877263581489), np.float64(0.5992374971966808), np.float64(0.6724402894281872)]\n",
            " Mean AUC: 0.6495\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 200 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 200\n",
            "Fold_0 - AUC: 0.6726 | Duration: 0.37s\n",
            "Fold_1 - AUC: 0.6515 | Duration: 0.62s\n",
            "Fold_2 - AUC: 0.6675 | Duration: 0.36s\n",
            "Fold_3 - AUC: 0.6123 | Duration: 0.33s\n",
            "Fold_4 - AUC: 0.6861 | Duration: 0.35s\n",
            "\n",
            " AUC Scores: [np.float64(0.6726047537368292), np.float64(0.6515396691220867), np.float64(0.6674501554783245), np.float64(0.6123149809374299), np.float64(0.6860632643913684)]\n",
            " Mean AUC: 0.6580\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 300 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 300\n",
            "Fold_0 - AUC: 0.6509 | Duration: 0.40s\n",
            "Fold_1 - AUC: 0.6605 | Duration: 0.43s\n",
            "Fold_2 - AUC: 0.6141 | Duration: 0.35s\n",
            "Fold_3 - AUC: 0.6026 | Duration: 0.46s\n",
            "Fold_4 - AUC: 0.6836 | Duration: 0.46s\n",
            "\n",
            " AUC Scores: [np.float64(0.650869884832149), np.float64(0.6604999396208188), np.float64(0.6141485275288092), np.float64(0.6026154967481497), np.float64(0.6836140103733113)]\n",
            " Mean AUC: 0.6423\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 500 features from merged training data.\n",
            "SHAP feature selection for TOP_N = 500\n",
            "Fold_0 - AUC: 0.6348 | Duration: 1.34s\n",
            "Fold_1 - AUC: 0.6288 | Duration: 1.43s\n",
            "Fold_2 - AUC: 0.5603 | Duration: 1.26s\n",
            "Fold_3 - AUC: 0.6194 | Duration: 1.20s\n",
            "Fold_4 - AUC: 0.5992 | Duration: 1.29s\n",
            "\n",
            " AUC Scores: [np.float64(0.6347708894878706), np.float64(0.6288370969689652), np.float64(0.5603255899030547), np.float64(0.619365328549002), np.float64(0.5991627713389256)]\n",
            " Mean AUC: 0.6085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SHAP: Discussion\n",
        "\n",
        "The SHAP-based feature selection showed a clear trend of improved performance with the inclusion of more top-ranked features, reaching optimal AUC scores in the range of 40 to 100 features. Performance peaked around 60200 features, after which it slightly declined, suggesting possible overfitting or inclusion of less relevant features. This demonstrates the importance of selecting an appropriate subset rather than relying on all available features. We will recap this discussion in the later section .\n",
        "\n",
        "SHAPs model-agnostic interpretability and ability to capture non-linear relationships contributed significantly to identifying impactful predictors. Importantly, to avoid data leakage, SHAP values were computed using only the merged training folds to ensure fair evaluation."
      ],
      "metadata": {
        "id": "Ctis6MXE9T0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest-Based Feature Selection\n",
        "\n",
        "In this section, we use a Random Forest classifier to rank features by their importance scores. Random Forest is a powerful tree-based ensemble method that can capture nonlinear relationships and interactions between features, suitable for importance estimation.\n",
        "\n",
        "To avoid data leakage, feature importance is computed only on the **merged training folds** derived from `StratifiedGroupKFold` with a fixed random seed. This ensures that the test data is completely excluded from the feature selection process, preserving the validity of subsequent evaluations.\n",
        "\n",
        "We then select the top-N features based on the learned importances and evaluate model performance using cross-validation.\n"
      ],
      "metadata": {
        "id": "bKU0ye5q98mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def get_rf_feature_ranking(X, y, max_samples=1000, random_state=42):\n",
        "    \"\"\"\n",
        "    Train a Random Forest classifier and rank features by their importance scores.\n",
        "\n",
        "    Parameters:\n",
        "        X (pd.DataFrame): Input feature matrix.\n",
        "        y (array-like): Target labels.\n",
        "        max_samples (int): Unused here but included for API consistency.\n",
        "        random_state (int): Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Features ranked by importance (descending order) based on Random Forest.\n",
        "    \"\"\"\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=-1)\n",
        "    rf.fit(X, y)\n",
        "    importances = rf.feature_importances_\n",
        "    return pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "def select_top_features_by_rf(X, y, top_n: int = 20) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Select the top-N most important features using Random Forest importance scores.\n",
        "\n",
        "    Parameters:\n",
        "        X (pd.DataFrame): Input feature matrix.\n",
        "        y (array-like): Target labels.\n",
        "        top_n (int): Number of top features to select.\n",
        "        random_state (int): Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Reduced feature matrix with top-N Random Forest-ranked features.\n",
        "    \"\"\"\n",
        "    ranking = get_rf_feature_ranking(X, y)\n",
        "    top_features = ranking.head(top_n).index\n",
        "    return X[top_features]"
      ],
      "metadata": {
        "id": "CfKJDZPI9uze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N_list = [10, 20, 25, 30, 35, 40, 45, 50, 100, 200, 300]\n",
        "\n",
        "#directory for saving values\n",
        "result_dir = \"assignment2_rf\"\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "# Loop over each TOP_N value and feature set\n",
        "\n",
        "for TOP_N in TOP_N_list:\n",
        "    for fs_name, X_base in feature_sets.items():\n",
        "\n",
        "        # Skip if not enough features\n",
        "        if X_base.shape[1] < TOP_N:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n Feature Set: {fs_name}\")\n",
        "\n",
        "        # ----------------------------\n",
        "        # Get all training data across folds using StratifiedGroupKFold\n",
        "        # This simulates what training data looks like across CV splits,\n",
        "        # allowing RF feature selection without touching any test fold.\n",
        "        # ----------------------------\n",
        "        cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        train_indices = []\n",
        "\n",
        "        for train_idx, _ in cv.split(X_base, features[\"y\"], features[\"groups\"]):\n",
        "            train_indices.extend(train_idx)\n",
        "\n",
        "        # Remove duplicates from aggregated training indices\n",
        "        unique_train_indices = sorted(set(train_indices))\n",
        "\n",
        "        # Extract merged training data for SHAP feature selection\n",
        "        X_train_merged = X_base.iloc[unique_train_indices]\n",
        "        y_train_merged = features[\"y\"][unique_train_indices]\n",
        "\n",
        "        # Run SHAP feature selection on merged training data\n",
        "        X_selected = select_top_features_by_rf(X_train_merged, y_train_merged, top_n=TOP_N)\n",
        "        selected_features = X_selected.columns.tolist()\n",
        "        selected_features.sort()\n",
        "\n",
        "        print(f\"Selected {len(selected_features)} features from merged training data.\")\n",
        "        print(f\"RF feature selection for TOP_N = {TOP_N}\")\n",
        "\n",
        "        # Apply selected features to full dataset for evaluation\n",
        "        X_shap = X_base[selected_features]\n",
        "\n",
        "        # Identify categorical columns\n",
        "        cat_cols = X_shap.columns[X_shap.dtypes == bool]\n",
        "\n",
        "        # Run model evaluation using existing CV pipeline\n",
        "        results = perform_cross_validation(\n",
        "            X_shap, features[\"y\"], features[\"groups\"],\n",
        "            estimator=estimator,\n",
        "            cats=cat_cols,\n",
        "            normalize=True,\n",
        "            select=None,  # SHAP already selects\n",
        "            oversample=True,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Save evaluation results\n",
        "        auc_scores = [r.metrics[\"AUC\"] for r in results]\n",
        "        avg_auc = np.mean(auc_scores)\n",
        "\n",
        "        print(f\"\\n AUC Scores: {auc_scores}\")\n",
        "        print(f\" Mean AUC: {avg_auc:.4f}\")\n",
        "\n",
        "        pd.DataFrame({\n",
        "            \"AUC Scores\": auc_scores,\n",
        "            \"Mean AUC\": [avg_auc] * len(auc_scores)\n",
        "        }).to_csv(os.path.join(result_dir, f\"{fs_name}__top{TOP_N}.csv\"), index=False)\n",
        "\n",
        "        # Save selected feature names\n",
        "        pd.Series(selected_features).to_csv(\n",
        "            os.path.join(result_dir, f\"{fs_name}__top{TOP_N}_features.csv\"),\n",
        "            index=False, header=False\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPFtMBKbADTI",
        "outputId": "eb7e4e2b-97a8-443e-c439-f1ea4b15b175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature Set: all_features\n",
            "Selected 10 features from merged training data.\n",
            "RF feature selection for TOP_N = 10\n",
            "Fold_0 - AUC: 0.6121 | Duration: 0.14s\n",
            "Fold_1 - AUC: 0.5725 | Duration: 0.10s\n",
            "Fold_2 - AUC: 0.6290 | Duration: 0.09s\n",
            "Fold_3 - AUC: 0.5746 | Duration: 0.11s\n",
            "Fold_4 - AUC: 0.6419 | Duration: 0.16s\n",
            "\n",
            " AUC Scores: [np.float64(0.6120803724577308), np.float64(0.5724550175099625), np.float64(0.6290012804097311), np.float64(0.5746103386409509), np.float64(0.6418966510853557)]\n",
            " Mean AUC: 0.6060\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 20 features from merged training data.\n",
            "RF feature selection for TOP_N = 20\n",
            "Fold_0 - AUC: 0.6382 | Duration: 0.13s\n",
            "Fold_1 - AUC: 0.6027 | Duration: 0.12s\n",
            "Fold_2 - AUC: 0.6024 | Duration: 0.12s\n",
            "Fold_3 - AUC: 0.5776 | Duration: 0.11s\n",
            "Fold_4 - AUC: 0.6236 | Duration: 0.12s\n",
            "\n",
            " AUC Scores: [np.float64(0.6381524136241118), np.float64(0.6026989494022461), np.float64(0.6023779037863545), np.float64(0.5776098901098901), np.float64(0.6236232951271051)]\n",
            " Mean AUC: 0.6089\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 25 features from merged training data.\n",
            "RF feature selection for TOP_N = 25\n",
            "Fold_0 - AUC: 0.6208 | Duration: 0.16s\n",
            "Fold_1 - AUC: 0.6196 | Duration: 0.15s\n",
            "Fold_2 - AUC: 0.6055 | Duration: 0.19s\n",
            "Fold_3 - AUC: 0.5994 | Duration: 0.10s\n",
            "Fold_4 - AUC: 0.6171 | Duration: 0.17s\n",
            "\n",
            " AUC Scores: [np.float64(0.6208282283753982), np.float64(0.6195507788914383), np.float64(0.6054874702762028), np.float64(0.5993706548553488), np.float64(0.6170519305884613)]\n",
            " Mean AUC: 0.6125\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 30 features from merged training data.\n",
            "RF feature selection for TOP_N = 30\n",
            "Fold_0 - AUC: 0.6278 | Duration: 0.18s\n",
            "Fold_1 - AUC: 0.6435 | Duration: 0.20s\n",
            "Fold_2 - AUC: 0.6428 | Duration: 0.13s\n",
            "Fold_3 - AUC: 0.5845 | Duration: 0.15s\n",
            "Fold_4 - AUC: 0.6318 | Duration: 0.20s\n",
            "\n",
            " AUC Scores: [np.float64(0.6277873070325901), np.float64(0.6434730105059775), np.float64(0.6427748308029999), np.float64(0.5845481049562682), np.float64(0.6318114874815907)]\n",
            " Mean AUC: 0.6261\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 35 features from merged training data.\n",
            "RF feature selection for TOP_N = 35\n",
            "Fold_0 - AUC: 0.6182 | Duration: 0.12s\n",
            "Fold_1 - AUC: 0.6256 | Duration: 0.22s\n",
            "Fold_2 - AUC: 0.6032 | Duration: 0.18s\n",
            "Fold_3 - AUC: 0.6033 | Duration: 0.11s\n",
            "Fold_4 - AUC: 0.6189 | Duration: 0.15s\n",
            "\n",
            " AUC Scores: [np.float64(0.6182308257779956), np.float64(0.6255766211810168), np.float64(0.6032010243277849), np.float64(0.6032602601480153), np.float64(0.6188928731510533)]\n",
            " Mean AUC: 0.6138\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 40 features from merged training data.\n",
            "RF feature selection for TOP_N = 40\n",
            "Fold_0 - AUC: 0.5738 | Duration: 0.17s\n",
            "Fold_1 - AUC: 0.6007 | Duration: 0.19s\n",
            "Fold_2 - AUC: 0.6014 | Duration: 0.21s\n",
            "Fold_3 - AUC: 0.5634 | Duration: 0.22s\n",
            "Fold_4 - AUC: 0.5999 | Duration: 0.19s\n",
            "\n",
            " AUC Scores: [np.float64(0.5738299436412644), np.float64(0.6006581330757155), np.float64(0.6014175964880191), np.float64(0.5633550123346042), np.float64(0.5999071524620605)]\n",
            " Mean AUC: 0.5878\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 45 features from merged training data.\n",
            "RF feature selection for TOP_N = 45\n",
            "Fold_0 - AUC: 0.6253 | Duration: 0.19s\n",
            "Fold_1 - AUC: 0.5951 | Duration: 0.16s\n",
            "Fold_2 - AUC: 0.6252 | Duration: 0.21s\n",
            "Fold_3 - AUC: 0.5608 | Duration: 0.19s\n",
            "Fold_4 - AUC: 0.6454 | Duration: 0.15s\n",
            "\n",
            " AUC Scores: [np.float64(0.6253369272237197), np.float64(0.595054945054945), np.float64(0.625224071702945), np.float64(0.5607899753307917), np.float64(0.6453704296599858)]\n",
            " Mean AUC: 0.6104\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 50 features from merged training data.\n",
            "RF feature selection for TOP_N = 50\n",
            "Fold_0 - AUC: 0.6136 | Duration: 0.26s\n",
            "Fold_1 - AUC: 0.6250 | Duration: 0.16s\n",
            "Fold_2 - AUC: 0.5984 | Duration: 0.19s\n",
            "Fold_3 - AUC: 0.5856 | Duration: 0.16s\n",
            "Fold_4 - AUC: 0.6457 | Duration: 0.16s\n",
            "\n",
            " AUC Scores: [np.float64(0.6136486155354081), np.float64(0.6249788672865597), np.float64(0.5983994878361075), np.float64(0.5855993496299619), np.float64(0.6457466222706025)]\n",
            " Mean AUC: 0.6137\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 100 features from merged training data.\n",
            "RF feature selection for TOP_N = 100\n",
            "Fold_0 - AUC: 0.6127 | Duration: 0.25s\n",
            "Fold_1 - AUC: 0.6280 | Duration: 0.24s\n",
            "Fold_2 - AUC: 0.5884 | Duration: 0.19s\n",
            "Fold_3 - AUC: 0.5496 | Duration: 0.22s\n",
            "Fold_4 - AUC: 0.6534 | Duration: 0.28s\n",
            "\n",
            " AUC Scores: [np.float64(0.6127174712080373), np.float64(0.627985750513223), np.float64(0.5884031461496251), np.float64(0.5496047320026911), np.float64(0.6533905359544087)]\n",
            " Mean AUC: 0.6064\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 200 features from merged training data.\n",
            "RF feature selection for TOP_N = 200\n",
            "Fold_0 - AUC: 0.6370 | Duration: 0.34s\n",
            "Fold_1 - AUC: 0.5842 | Duration: 0.32s\n",
            "Fold_2 - AUC: 0.6254 | Duration: 0.25s\n",
            "Fold_3 - AUC: 0.5696 | Duration: 0.25s\n",
            "Fold_4 - AUC: 0.6143 | Duration: 0.37s\n",
            "\n",
            " AUC Scores: [np.float64(0.636976231315854), np.float64(0.5841927303465765), np.float64(0.6253521126760564), np.float64(0.5695503476115722), np.float64(0.614282512646475)]\n",
            " Mean AUC: 0.6061\n",
            "\n",
            " Feature Set: all_features\n",
            "Selected 300 features from merged training data.\n",
            "RF feature selection for TOP_N = 300\n",
            "Fold_0 - AUC: 0.6166 | Duration: 0.50s\n",
            "Fold_1 - AUC: 0.5870 | Duration: 0.45s\n",
            "Fold_2 - AUC: 0.6037 | Duration: 0.42s\n",
            "Fold_3 - AUC: 0.5723 | Duration: 0.40s\n",
            "Fold_4 - AUC: 0.6457 | Duration: 0.39s\n",
            "\n",
            " AUC Scores: [np.float64(0.6165890713060524), np.float64(0.5870064001932135), np.float64(0.6037497713554052), np.float64(0.5722555505718772), np.float64(0.6456825894858167)]\n",
            " Mean AUC: 0.6051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Feature Selection: Discussion\n",
        "\n",
        "Random Forest-based feature selection was applied to the full feature set, and performance was evaluated across different top-N feature thresholds. The highest AUC (0.6261) was observed with the top 30 features, but overall, performance remained fairly stable around the 0.600.61 range.\n",
        "\n",
        "Unlike SHAP, RF did not show consistent improvement with more features, possibly due to noise accumulation. As with SHAP, care was taken to avoid data leakage by computing feature importance only on merged training folds.\n"
      ],
      "metadata": {
        "id": "RqI9_darCqN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 3. Please try using hyperopt for model hyperparameter tuning (20 pts)"
      ],
      "metadata": {
        "id": "Q_FJZurk3ulY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: Please be aware that for revised xgboost classifier EvXGBClassifier, there exist other parameters other than default XGBClassifier parameters such as eval_size.\n",
        "\n",
        "For hyperparameter tuning, we will use 20% of training set as validation set to avoid data leakage.\n",
        "\n",
        "If it is too timeconsuming to run the code in colab, please run the code locally and consider using [ray tune](https://docs.ray.io/en/latest/tune/index.html) if needed."
      ],
      "metadata": {
        "id": "0gQcs-te34PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_sets = {\n",
        "    \"all_features\": features[\"X_cleaned\"],\n",
        "}"
      ],
      "metadata": {
        "id": "t0lfPs04DH8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import STATUS_OK, Trials, hp, fmin, tpe, base\n",
        "\n",
        "#load shape features from all_features__top50_features.csv and assign in features from X_cleaned\n",
        "shap_features = pd.read_csv(\"/content/assignment2_shap/all_features__top60_features.csv\", index_col=0)\n",
        "\n",
        "\n",
        "X_selected = features[\"X_cleaned\"].loc[:, shap_features.index].copy()\n",
        "\n",
        "\n",
        "\n",
        "# define your outer CV\n",
        "OUTER_CV = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def objective(params):\n",
        "    val_scores = []\n",
        "\n",
        "    # outer loop: split into train_full / test (we will only use train_full for tuning)\n",
        "    for train_full_idx, _ in OUTER_CV.split(X_selected, y, groups):\n",
        "        X_train_full = X_selected.iloc[train_full_idx]\n",
        "        y_train_full = y[train_full_idx]\n",
        "\n",
        "        # split 20% of the *training fold* into a validation set\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full,\n",
        "            test_size=0.20,\n",
        "            stratify=y_train_full,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Normalize\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "        # (Optional) Oversample on *training only*\n",
        "        if np.any(X_train_scaled[:, -1] < 1):\n",
        "            smote = SMOTENC(\n",
        "                categorical_features=[X_train_scaled.shape[1]-1],\n",
        "                random_state=int(params['random_state'])\n",
        "            )\n",
        "        else:\n",
        "            smote = SMOTE(random_state=int(params['random_state']))\n",
        "        X_train_os, y_train_os = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        # Feature selection disabled\n",
        "        X_train_sel = X_train_os\n",
        "        X_val_sel   = X_val_scaled\n",
        "\n",
        "        # Train & score on *validation only*\n",
        "        clf = EvXGBClassifier(\n",
        "          random_state=int(params['random_state']),\n",
        "          eval_metric='logloss',\n",
        "          max_depth=params['max_depth'],\n",
        "          learning_rate=params['learning_rate'],\n",
        "          min_child_weight=params['min_child_weight'],\n",
        "          subsample=params['subsample'],\n",
        "          colsample_bytree=params['colsample_bytree'],\n",
        "          gamma=params['gamma'],\n",
        "          n_estimators=int(params['n_estimators']),\n",
        "          reg_alpha=params['reg_alpha'],\n",
        "          reg_lambda=params['reg_lambda'],\n",
        "        )\n",
        "\n",
        "        clf.fit(X_train_sel, y_train_os)\n",
        "\n",
        "\n",
        "        y_val_prob = clf.predict_proba(X_val_sel)[:, 1]\n",
        "        val_scores.append(roc_auc_score(y_val, y_val_prob))\n",
        "\n",
        "    # Hyperopt minimizes loss, so negate AUC\n",
        "    return {'loss': -np.mean(val_scores), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "# Define hyperparameter space\n",
        "# define your search space (fill in any missing parameters e.g. max_depth)\n",
        "space = {\n",
        "    'max_depth': hp.choice('max_depth', list(range(3, 10))),\n",
        "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
        "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
        "    'gamma': hp.uniform('gamma', 0, 5),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.3)),\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 300, 10),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# run hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=100,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "#save the parameters in a file\n",
        "best_params = {k: v for k, v in best.items() if k != 'random_state'}\n",
        "best_params['max_depth'] = [3, 4, 5, 6, 7, 8, 9][best_params['max_depth']]\n",
        "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
        "best_params['min_child_weight'] = int(best_params['min_child_weight'])\n",
        "best_params['random_state'] = 42\n",
        "result_dir = os.path.join(\"assignment3\", \"hyperopt\")\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "pd.DataFrame([best_params]).to_csv(os.path.join(result_dir, \"best_hyperparameters.csv\"), index=False)"
      ],
      "metadata": {
        "id": "-8PyEHuv4R-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714d06d6-9499-4940-b590-3cfc54dabf42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|| 100/100 [04:10<00:00,  2.50s/trial, best loss: -0.7371891925366012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#open from parameter directory\n",
        "param = pd.read_csv('/content/assignment3/hyperopt/best_hyperparameters.csv')\n",
        "\n",
        "#print the parameters as a list\n",
        "print(param.to_dict(orient='records'))\n",
        "\n",
        "cat_cols = X_selected.columns[X_selected.dtypes == bool]\n",
        "\n",
        "#hyperparameter tuned model\n",
        "base_estimator = EvXGBClassifier(\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic',\n",
        "    verbosity=0,\n",
        "    learning_rate= param['learning_rate'],\n",
        "    colsample_bytree= param['colsample_bytree'],\n",
        "    gamma= param['gamma'],\n",
        "    max_depth=param['max_depth'],\n",
        "    min_child_weight=param['min_child_weight'],\n",
        "    n_estimators=param['n_estimators'],\n",
        "    reg_alpha=param['reg_alpha'],\n",
        "    reg_lambda=param['reg_lambda'],\n",
        "    subsample=param['subsample']\n",
        ")\n",
        "\n",
        "#Perform cross validation including model training and evaluation\n",
        "results = perform_cross_validation(\n",
        "        X, features[\"y\"], features[\"groups\"],\n",
        "        estimator=base_estimator,\n",
        "        cats=cat_cols,\n",
        "        normalize=True,\n",
        "        select=None,\n",
        "        oversample=True,\n",
        "        random_state=42\n",
        "    )\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xII4KDfZIrVt",
        "outputId": "d3014eef-4977-4b18-f664-c794cb929e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'colsample_bytree': 0.8463935920953023, 'gamma': 0.2144097629496495, 'learning_rate': 0.0736917612203914, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 280, 'reg_alpha': 0.4775914535854724, 'reg_lambda': 0.6862888539325457, 'subsample': 0.7290628198195405, 'random_state': 42}]\n",
            "Fold_0 - AUC: 0.5927 | Duration: 0.88s\n",
            "Fold_1 - AUC: 0.5278 | Duration: 0.78s\n",
            "Fold_2 - AUC: 0.5604 | Duration: 0.72s\n",
            "Fold_3 - AUC: 0.5458 | Duration: 1.21s\n",
            "Fold_4 - AUC: 0.6047 | Duration: 0.79s\n",
            "0.5663015221318979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 4. Please consider replacing the previous traditional machine learning model with deep learning models designed for **tabular data** to improve model performance. (20 pts)"
      ],
      "metadata": {
        "id": "EPz966iD6yoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: Since features are already extracted manually, it is impossible to use end-to-end deep learning models. Instead, try replacing xgboost with deep learning models designed for **tabular data** and see if there is performance improvement."
      ],
      "metadata": {
        "id": "oIfbskLs7qmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may need to change runtime to TPU first to use torch or other packages you may want to use.\n",
        "\n"
      ],
      "metadata": {
        "id": "CbE_mod6TUIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Please compare it with your previous XGBoost model performance and think about why it is higher or lower than XGBoost."
      ],
      "metadata": {
        "id": "DDEeP0pkBsn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TabNet\n",
        "We replace the previously used XGBoost model with a deep learning model specifically designed for tabular data: TabNet. As our features are already manually extracted, end-to-end deep learning pipelines are not applicable. Instead, TabNet is employed as a drop-in replacement for XGBoost to assess if a deep learning-based architecture can improve classification performance on manually engineered tabular features."
      ],
      "metadata": {
        "id": "5Ta7DaxoP9kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######Your code for deep learning model#########\n",
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "id": "MZk2A2-I7nPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d8c1bd-9c98-412a-8519-d7318dbb026d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-tabnet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do the necessary feature format conversion for the work. Here we choose three different datasets for trials:\n",
        "\n",
        "*   SHAP Top 60: Selected based on SHAP (EvXGBoost), having best pre-tuning AUC.\n",
        "*   All Features: Based on the assumption that deep learning handles feature selection implicitly via regularization (neuron dropout/pruning)\n",
        "*   Baseline Features\n",
        "\n"
      ],
      "metadata": {
        "id": "Kxy9oBZuQ9dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature selection\n",
        "\n",
        "X_deep = X_selected.copy(deep=True) #Shap Top 60 features\n",
        "# X_deep = X_cleaned.copy(deep=True)\n",
        "# X_deep = X.copy(deep=True) #The full feature set\n",
        "y_deep = y\n",
        "\n",
        "cat_cols = list(X_deep.columns[X_deep.dtypes ==  bool])\n",
        "num_cols = list(X_deep.columns[~X_deep.columns.isin(cat_cols)])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_deep[num_cols] = scaler.fit_transform(X_deep[num_cols])\n",
        "\n",
        "#convert categorical booleant to float\n",
        "X_deep[cat_cols] = X_deep[cat_cols].astype(float)\n",
        "\n",
        "#convert to numpy\n",
        "X_np = X_deep.to_numpy()\n",
        "y_np = y"
      ],
      "metadata": {
        "id": "0mBAoJKYEZxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the model for training"
      ],
      "metadata": {
        "id": "2sDlzHybtwPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "# Initialize a cross-validation strategy that maintains class distribution and group integrity\n",
        "crossvalidation = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store evaluation results from each fold\n",
        "fold_results = []\n",
        "\n",
        "# Loop over each fold provided by the cross-validation split\n",
        "for fold, (train_idx, test_idx) in enumerate(crossvalidation.split(X_np, y_np, groups)):\n",
        "    print(f\"\\n Fold {fold + 1}\")\n",
        "\n",
        "    # Split the dataset into training and testing sets based on current fold indices\n",
        "    X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
        "    y_train, y_test = y_np[train_idx], y_np[test_idx]\n",
        "\n",
        "    # Initialize the TabNetClassifier with default hyperparameters\n",
        "    model = TabNetClassifier(\n",
        "        device_name='cuda',\n",
        "        n_d=4,\n",
        "        n_a=4,\n",
        "        n_steps=5,\n",
        "        gamma=1.5,\n",
        "        lambda_sparse=1e-4,\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=2e-2),\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        scheduler_params=dict(gamma=0.95, step_size=20),\n",
        "        mask_type='sparsemax',\n",
        "        verbose=0,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Train the model with early stopping and evaluation metrics\n",
        "    model.fit(\n",
        "        X_train=X_train, y_train=y_train,\n",
        "        eval_set=[(X_test, y_test)],\n",
        "        eval_name=[\"val\"],\n",
        "        eval_metric=[\"auc\"],\n",
        "        max_epochs=200,\n",
        "        patience=20,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128,\n",
        "    )\n",
        "\n",
        "    # Generate predictions and predicted probabilities for the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate evaluation metrics: Accuracy, F1 Score, and AUC\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Display fold-specific evaluation metrics\n",
        "    print(f\"Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "\n",
        "    # Store the metrics for this fold\n",
        "    fold_results.append(dict(Accuracy=acc, F1=f1, AUC=auc))\n",
        "\n",
        "# Create a DataFrame from the results and display the average performance across all folds\n",
        "df = pd.DataFrame(fold_results)\n",
        "\n",
        "save_dir = './assignment4/tabnet'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "df.to_csv(os.path.join(save_dir, 'tabnet_results.csv'), index=False)\n",
        "\n",
        "\n",
        "print(\"\\n Final Mean Scores:\")\n",
        "print(df.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPl-y-hq3mFv",
        "outputId": "034f80df-134b-469d-a1bd-7e488aaf2f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold 1\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_auc = 0.70635\n",
            "Accuracy: 0.7136 | F1: 0.5349 | AUC: 0.7063\n",
            "\n",
            " Fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 55 and best_val_auc = 0.6087\n",
            "Accuracy: 0.5763 | F1: 0.1570 | AUC: 0.6087\n",
            "\n",
            " Fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_auc = 0.67783\n",
            "Accuracy: 0.7078 | F1: 0.3889 | AUC: 0.6778\n",
            "\n",
            " Fold 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_auc = 0.63436\n",
            "Accuracy: 0.6533 | F1: 0.3475 | AUC: 0.6344\n",
            "\n",
            " Fold 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_auc = 0.66853\n",
            "Accuracy: 0.6415 | F1: 0.2629 | AUC: 0.6685\n",
            "\n",
            " Final Mean Scores:\n",
            "Accuracy    0.658499\n",
            "F1          0.338252\n",
            "AUC         0.659155\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test the hyperparamters\n",
        "\n",
        "#load the hyperparameters\n",
        "param = pd.read_csv('/content/assignment4/hyperopt/best_hyperparameters.csv')\n",
        "\n",
        "# Initialize a cross-validation strategy that maintains class distribution and group integrity\n",
        "crossvalidation = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store evaluation results from each fold\n",
        "fold_results = []\n",
        "\n",
        "# Loop over each fold provided by the cross-validation split\n",
        "for fold, (train_idx, test_idx) in enumerate(crossvalidation.split(X_np, y_np, groups)):\n",
        "    print(f\"\\n Fold {fold + 1}\")\n",
        "\n",
        "    # Split the dataset into training and testing sets based on current fold indices\n",
        "    X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
        "    y_train, y_test = y_np[train_idx], y_np[test_idx]\n",
        "\n",
        "    # Initialize the TabNetClassifier with default hyperparameters\n",
        "    model = TabNetClassifier(\n",
        "        device_name='cuda',\n",
        "        n_d=int(param['n_d']),\n",
        "        n_a=int(param['n_a']),\n",
        "        n_steps=int(param['n_steps']),\n",
        "        gamma=float(param['gamma']),\n",
        "        lambda_sparse=float(param['lambda_sparse']),\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=float(param['2e^-2'])),\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        scheduler_params=dict(gamma=0.95, step_size=20),\n",
        "        mask_type='sparsemax',\n",
        "        verbose=0,\n",
        "        seed=int(param['random_state'])\n",
        "    )\n",
        "\n",
        "\n",
        "    # Train the model with early stopping and evaluation metrics\n",
        "    model.fit(\n",
        "        X_train=X_train, y_train=y_train,\n",
        "        eval_set=[(X_test, y_test)],\n",
        "        eval_name=[\"val\"],\n",
        "        eval_metric=[\"auc\"],\n",
        "        max_epochs=200,\n",
        "        patience=20,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128,\n",
        "    )\n",
        "\n",
        "    # Generate predictions and predicted probabilities for the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate evaluation metrics: Accuracy, F1 Score, and AUC\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Display fold-specific evaluation metrics\n",
        "    print(f\"Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "\n",
        "    # Store the metrics for this fold\n",
        "    fold_results.append(dict(Accuracy=acc, F1=f1, AUC=auc))\n",
        "\n",
        "# Create a DataFrame from the results and display the average performance across all folds\n",
        "df = pd.DataFrame(fold_results)\n",
        "\n",
        "save_dir = './assignment4/tabnet'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "df.to_csv(os.path.join(save_dir, 'tabnet_results.csv'), index=False)\n",
        "\n",
        "\n",
        "print(\"\\n Final Mean Scores:\")\n",
        "print(df.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-2AgVQMdOH4",
        "outputId": "87ae7a99-3c03-425a-ab1a-631d55087938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-107-2000578261>:23: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_d=int(param['n_d']),\n",
            "<ipython-input-107-2000578261>:24: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_a=int(param['n_a']),\n",
            "<ipython-input-107-2000578261>:25: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_steps=int(param['n_steps']),\n",
            "<ipython-input-107-2000578261>:26: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  gamma=float(param['gamma']),\n",
            "<ipython-input-107-2000578261>:27: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  lambda_sparse=float(param['lambda_sparse']),\n",
            "<ipython-input-107-2000578261>:29: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  optimizer_params=dict(lr=float(param['lr'])),\n",
            "<ipython-input-107-2000578261>:34: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  seed=int(param['random_state'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_auc = 0.62079\n",
            "Accuracy: 0.6468 | F1: 0.4436 | AUC: 0.6208\n",
            "\n",
            " Fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-107-2000578261>:23: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_d=int(param['n_d']),\n",
            "<ipython-input-107-2000578261>:24: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_a=int(param['n_a']),\n",
            "<ipython-input-107-2000578261>:25: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_steps=int(param['n_steps']),\n",
            "<ipython-input-107-2000578261>:26: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  gamma=float(param['gamma']),\n",
            "<ipython-input-107-2000578261>:27: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  lambda_sparse=float(param['lambda_sparse']),\n",
            "<ipython-input-107-2000578261>:29: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  optimizer_params=dict(lr=float(param['lr'])),\n",
            "<ipython-input-107-2000578261>:34: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  seed=int(param['random_state'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 45 and best_val_auc = 0.58334\n",
            "Accuracy: 0.5849 | F1: 0.0242 | AUC: 0.5833\n",
            "\n",
            " Fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-107-2000578261>:23: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_d=int(param['n_d']),\n",
            "<ipython-input-107-2000578261>:24: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_a=int(param['n_a']),\n",
            "<ipython-input-107-2000578261>:25: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_steps=int(param['n_steps']),\n",
            "<ipython-input-107-2000578261>:26: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  gamma=float(param['gamma']),\n",
            "<ipython-input-107-2000578261>:27: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  lambda_sparse=float(param['lambda_sparse']),\n",
            "<ipython-input-107-2000578261>:29: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  optimizer_params=dict(lr=float(param['lr'])),\n",
            "<ipython-input-107-2000578261>:34: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  seed=int(param['random_state'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_auc = 0.58251\n",
            "Accuracy: 0.7154 | F1: 0.0625 | AUC: 0.5825\n",
            "\n",
            " Fold 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-107-2000578261>:23: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_d=int(param['n_d']),\n",
            "<ipython-input-107-2000578261>:24: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_a=int(param['n_a']),\n",
            "<ipython-input-107-2000578261>:25: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_steps=int(param['n_steps']),\n",
            "<ipython-input-107-2000578261>:26: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  gamma=float(param['gamma']),\n",
            "<ipython-input-107-2000578261>:27: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  lambda_sparse=float(param['lambda_sparse']),\n",
            "<ipython-input-107-2000578261>:29: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  optimizer_params=dict(lr=float(param['lr'])),\n",
            "<ipython-input-107-2000578261>:34: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  seed=int(param['random_state'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_auc = 0.5665\n",
            "Accuracy: 0.6202 | F1: 0.3550 | AUC: 0.5665\n",
            "\n",
            " Fold 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "<ipython-input-107-2000578261>:23: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_d=int(param['n_d']),\n",
            "<ipython-input-107-2000578261>:24: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_a=int(param['n_a']),\n",
            "<ipython-input-107-2000578261>:25: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  n_steps=int(param['n_steps']),\n",
            "<ipython-input-107-2000578261>:26: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  gamma=float(param['gamma']),\n",
            "<ipython-input-107-2000578261>:27: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  lambda_sparse=float(param['lambda_sparse']),\n",
            "<ipython-input-107-2000578261>:29: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  optimizer_params=dict(lr=float(param['lr'])),\n",
            "<ipython-input-107-2000578261>:34: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  seed=int(param['random_state'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_auc = 0.54518\n",
            "Accuracy: 0.5407 | F1: 0.4205 | AUC: 0.5452\n",
            "\n",
            " Final Mean Scores:\n",
            "Accuracy    0.621592\n",
            "F1          0.261174\n",
            "AUC         0.579665\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 5. Please try combining all the above methods to push the model performance. (20 pts)"
      ],
      "metadata": {
        "id": "aZdspTz_G2D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble: EvXGBoost X TabNet\n",
        "\n",
        "The top 60 most important features were selected based on SHAP values from Assignment 2. These features capture the most influential patterns identified by prior model explanations.\n",
        "\n",
        "**Modeling Approach:**\n",
        "A soft voting ensemble is used to combine predictions from two different classifiers:\n",
        "\n",
        "* XGBoost (with hyperparameters optimized via cross-validation)\n",
        "\n",
        "* TabNet (also tuned using Hyperopt for best performance)\n",
        "\n",
        "Instead of using the final class predictions, we averaged their predicted probabilities:\n",
        "  \n",
        "  $$\n",
        "  \\text{final_proba} = w_1 \\cdot \\text{xgb_proba} + w_2 \\cdot \\text{tabnet_proba}\n",
        "  $$\n",
        "\n",
        "Why Soft Voting?\n",
        "\n",
        "\n",
        "Soft voting averages the predicted probabilities from each model rather than their final class labels. This approach:\n",
        "\n",
        "* Takes into account model confidence\n",
        "\n",
        "* Often improves overall predictive performance\n",
        "\n",
        "Reduces variance by balancing the strengths of different model types\n",
        "\n",
        "This ensemble leverages both tree-based and deep learning-based decision processes to create a more robust classifier."
      ],
      "metadata": {
        "id": "ftBi462fHGQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 5  Final Ensemble Model\n",
        "\n",
        "In the final stage, we employed a **soft voting ensemble** combining predictions from two models: `evXGBoost` and `TabNet`.\n",
        "\n",
        "-\n",
        "\n",
        "- We evaluated several weight combinations. The best performance was achieved with:\n",
        "  - **XGBoost Weight:** 0.6\n",
        "  - **TabNet Weight:** 0.4\n",
        "  - **Resulting AUC:** **0.682**\n",
        "\n",
        "This ensemble approach improved performance over individual models, demonstrating the benefit of combining complementary learners.\n"
      ],
      "metadata": {
        "id": "vk5VXcNFrHiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "results = []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(cv.split(X_np, y_np, groups)):\n",
        "    print(f\"\\nFold {fold + 1}\")\n",
        "    X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
        "    y_train, y_test = y_np[train_idx], y_np[test_idx]\n",
        "\n",
        "    # XGBoost model\n",
        "    xgb_model = EvXGBClassifier(\n",
        "      random_state=42,\n",
        "      eval_metric='logloss',\n",
        "      eval_size=0.2,\n",
        "      early_stopping_rounds=10,\n",
        "      objective='binary:logistic',\n",
        "      verbosity=0,\n",
        "      learning_rate=0.01\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # TabNet model\n",
        "    tabnet_model = TabNetClassifier(\n",
        "        device_name='cuda',\n",
        "        n_d=64,\n",
        "        n_a=64,\n",
        "        n_steps=5,\n",
        "        gamma=1.5,\n",
        "        lambda_sparse=1e-4,\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=2e-2),\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        scheduler_params=dict(gamma=0.95, step_size=20),\n",
        "        mask_type='sparsemax',\n",
        "        verbose=0,\n",
        "        seed=42\n",
        "    )\n",
        "    tabnet_model.fit(\n",
        "        X_train=X_train, y_train=y_train,\n",
        "        eval_set=[(X_test, y_test)],\n",
        "        eval_name=[\"val\"],\n",
        "        eval_metric=[\"auc\"],\n",
        "        max_epochs=200,\n",
        "        patience=20,\n",
        "        batch_size=512,\n",
        "        virtual_batch_size=128,\n",
        "    )\n",
        "    tabnet_proba = tabnet_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Soft voting ensemble\n",
        "    final_proba = 0.55 * xgb_proba + 0.45 * tabnet_proba #Here the parameters can be changed with attempts\n",
        "    final_preds = (final_proba >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, final_preds)\n",
        "    f1 = f1_score(y_test, final_preds)\n",
        "    auc = roc_auc_score(y_test, final_proba)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "    results.append(dict(Fold=fold + 1, Accuracy=acc, F1=f1, AUC=auc))\n",
        "\n",
        "# Summary\n",
        "df = pd.DataFrame(results)\n",
        "print(\"\\nFinal Mean Scores:\")\n",
        "print(df.mean())\n"
      ],
      "metadata": {
        "id": "T9SI2OpwG8eE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0178b4a4-3f8c-48c8-de1f-46c7f91427c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_auc = 0.7051\n",
            "Accuracy: 0.6516 | F1: 0.1412 | AUC: 0.7424\n",
            "\n",
            "Fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_auc = 0.63382\n",
            "Accuracy: 0.6244 | F1: 0.3540 | AUC: 0.6594\n",
            "\n",
            "Fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_val_auc = 0.65334\n",
            "Accuracy: 0.7249 | F1: 0.2564 | AUC: 0.6641\n",
            "\n",
            "Fold 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 3 and best_val_auc = 0.60205\n",
            "Accuracy: 0.6882 | F1: 0.3142 | AUC: 0.6529\n",
            "\n",
            "Fold 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_auc = 0.68313\n",
            "Accuracy: 0.6628 | F1: 0.3507 | AUC: 0.6918\n",
            "\n",
            "Final Mean Scores:\n",
            "Fold        3.000000\n",
            "Accuracy    0.670342\n",
            "F1          0.283298\n",
            "AUC         0.682125\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion\n",
        "\n",
        "By averaging their predicted probabilities (rather than using hard votes), we allowed the ensemble to capture richer confidence information from both models. We experimented with different weight combinations and found that assigning **0.55 to XGBoost and 0.45 to TabNet** yielded the best performance, achieving a **mean AUC of 0.682**.\n",
        "\n",
        "This confirms that ensemble learning, even with only two models, can effectively boost robustness and generalization by leveraging diverse modeling perspectives."
      ],
      "metadata": {
        "id": "g-FZ7lfKsMLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix"
      ],
      "metadata": {
        "id": "mF7bKOliuAUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning for TabNet"
      ],
      "metadata": {
        "id": "tCvSWciiuCpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Outer CV setup\n",
        "OUTER_CV = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define objective function for Hyperopt\n",
        "def objective(params):\n",
        "    val_scores = []\n",
        "\n",
        "    for train_full_idx, _ in OUTER_CV.split(X_np, y_np, groups):\n",
        "        X_train_full = X_np[train_full_idx]\n",
        "        y_train_full = y_np[train_full_idx]\n",
        "\n",
        "        # Inner split: train/val from training fold only\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full,\n",
        "            test_size=0.20,\n",
        "            stratify=y_train_full,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Initialize TabNet model with hyperparameters\n",
        "        model = TabNetClassifier(\n",
        "            n_d=int(params[\"n_d\"]),\n",
        "            n_a=int(params[\"n_a\"]),\n",
        "            n_steps=int(params[\"n_steps\"]),\n",
        "            gamma=params[\"gamma\"],\n",
        "            lambda_sparse=params[\"lambda_sparse\"],\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params=dict(lr=params[\"lr\"]),\n",
        "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "            scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
        "            mask_type=\"entmax\",\n",
        "            verbose=0,\n",
        "            device_name=\"cuda\"\n",
        "        )\n",
        "\n",
        "        # Train on training data, validate on val split\n",
        "        model.fit(\n",
        "            X_train=X_train,\n",
        "            y_train=y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric=[\"auc\"],\n",
        "            max_epochs=200,\n",
        "            patience=20,\n",
        "            batch_size=512,\n",
        "            virtual_batch_size=128\n",
        "        )\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        y_val_pred = model.predict_proba(X_val)[:, 1]\n",
        "        val_auc = roc_auc_score(y_val, y_val_pred)\n",
        "        val_scores.append(val_auc)\n",
        "\n",
        "    return {'loss': -np.mean(val_scores), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "# Define search space\n",
        "space = {\n",
        "    \"n_d\": hp.choice(\"n_d\", [32, 64, 128]),\n",
        "    \"n_a\": hp.choice(\"n_a\", [32, 64, 128]),\n",
        "    \"n_steps\": hp.choice(\"n_steps\", [3, 5, 7]),\n",
        "    \"gamma\": hp.uniform(\"gamma\", 1.0, 2.0),\n",
        "    \"lambda_sparse\": hp.loguniform(\"lambda_sparse\", np.log(1e-6), np.log(1e-2)),\n",
        "    \"lr\": hp.loguniform(\"lr\", np.log(1e-3), np.log(5e-2)),\n",
        "}\n",
        "\n",
        "# Run hyperparameter tuning with Hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=30,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "#processing parameters\n",
        "best_params = {k: v for k, v in best.items()}\n",
        "best_params[\"n_d\"] = [32, 64, 128][best_params[\"n_d\"]]\n",
        "best_params[\"n_a\"] = [32, 64, 128][best_params[\"n_a\"]]\n",
        "best_params[\"n_steps\"] = [3, 5, 7][best_params[\"n_steps\"]]\n",
        "best_params[\"lambda_sparse\"] = 10 ** best_params[\"lambda_sparse\"]\n",
        "best_params[\"lr\"] = 10 ** best_params[\"lr\"]\n",
        "best_params[\"random_state\"] = 42\n",
        "\n",
        "#saving parameters\n",
        "result_dir = os.path.join(\"assignment4\", \"hyperopt\")\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "pd.DataFrame([best_params]).to_csv(os.path.join(result_dir, \"best_hyperparameters.csv\"), index=False)\n",
        "print(list(best_params.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq9Pv8ssD6kE",
        "outputId": "ba10276b-eccf-40a2-b9da-d7c7e9b35eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.65599\n",
            "  0%|          | 0/30 [00:04<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 29 with best_epoch = 9 and best_val_0_auc = 0.64368\n",
            "  0%|          | 0/30 [00:07<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.63917\n",
            "  0%|          | 0/30 [00:12<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.70551\n",
            "  0%|          | 0/30 [00:20<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.65807\n",
            "  3%|         | 1/30 [00:26<12:39, 26.20s/trial, best loss: -0.6604850658656904]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_0_auc = 0.61946\n",
            "  3%|         | 1/30 [00:29<12:39, 26.20s/trial, best loss: -0.6604850658656904]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 47 and best_val_0_auc = 0.66357\n",
            "  3%|         | 1/30 [00:37<12:39, 26.20s/trial, best loss: -0.6604850658656904]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.66796\n",
            "  3%|         | 1/30 [00:41<12:39, 26.20s/trial, best loss: -0.6604850658656904]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.72213\n",
            "  3%|         | 1/30 [00:46<12:39, 26.20s/trial, best loss: -0.6604850658656904]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.66717\n",
            "  7%|         | 2/30 [00:52<12:19, 26.42s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.61998\n",
            "  7%|         | 2/30 [01:04<12:19, 26.42s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.61723\n",
            "  7%|         | 2/30 [01:11<12:19, 26.42s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 34 and best_val_0_auc = 0.65567\n",
            "  7%|         | 2/30 [01:23<12:19, 26.42s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_0_auc = 0.62123\n",
            "  7%|         | 2/30 [01:29<12:19, 26.42s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 56 with best_epoch = 36 and best_val_0_auc = 0.63211\n",
            " 10%|         | 3/30 [01:42<16:46, 37.28s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.64371\n",
            " 10%|         | 3/30 [01:55<16:46, 37.28s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_auc = 0.60714\n",
            " 10%|         | 3/30 [02:02<16:46, 37.28s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 52 and best_val_0_auc = 0.64413\n",
            " 10%|         | 3/30 [02:18<16:46, 37.28s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.65239\n",
            " 10%|         | 3/30 [02:25<16:46, 37.28s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.61877\n",
            " 13%|        | 4/30 [02:31<18:05, 41.74s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.64587\n",
            " 13%|        | 4/30 [02:39<18:05, 41.74s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_auc = 0.6836\n",
            " 13%|        | 4/30 [02:53<18:05, 41.74s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 55 and best_val_0_auc = 0.69885\n",
            " 13%|        | 4/30 [03:11<18:05, 41.74s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_auc = 0.67041\n",
            " 13%|        | 4/30 [03:17<18:05, 41.74s/trial, best loss: -0.6680572630115097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 34 and best_val_0_auc = 0.65525\n",
            " 17%|        | 5/30 [03:29<19:52, 47.71s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.61165\n",
            " 17%|        | 5/30 [03:35<19:52, 47.71s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.67554\n",
            " 17%|        | 5/30 [03:41<19:52, 47.71s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.6966\n",
            " 17%|        | 5/30 [03:46<19:52, 47.71s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.68739\n",
            " 17%|        | 5/30 [03:52<19:52, 47.71s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.6548\n",
            " 20%|        | 6/30 [03:57<16:20, 40.84s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_val_0_auc = 0.66261\n",
            " 20%|        | 6/30 [04:04<16:20, 40.84s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 85 with best_epoch = 65 and best_val_0_auc = 0.65779\n",
            " 20%|        | 6/30 [04:14<16:20, 40.84s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.58109\n",
            " 20%|        | 6/30 [04:18<16:20, 40.84s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_auc = 0.65514\n",
            " 20%|        | 6/30 [04:26<16:20, 40.84s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_0_auc = 0.64913\n",
            " 23%|       | 7/30 [04:34<15:14, 39.76s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.63993\n",
            " 23%|       | 7/30 [04:39<15:14, 39.76s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.64469\n",
            " 23%|       | 7/30 [04:43<15:14, 39.76s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_auc = 0.66305\n",
            " 23%|       | 7/30 [04:46<15:14, 39.76s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_auc = 0.6951\n",
            " 23%|       | 7/30 [04:50<15:14, 39.76s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.68408\n",
            " 27%|       | 8/30 [04:54<12:10, 33.20s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.61618\n",
            " 27%|       | 8/30 [05:06<12:10, 33.20s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 48 and best_val_0_auc = 0.66566\n",
            " 27%|       | 8/30 [05:21<12:10, 33.20s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 125 with best_epoch = 105 and best_val_0_auc = 0.66097\n",
            " 27%|       | 8/30 [05:49<12:10, 33.20s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_auc = 0.67329\n",
            " 27%|       | 8/30 [06:01<12:10, 33.20s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 77 with best_epoch = 57 and best_val_0_auc = 0.65057\n",
            " 30%|       | 9/30 [06:18<17:16, 49.38s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 90 with best_epoch = 70 and best_val_0_auc = 0.65435\n",
            " 30%|       | 9/30 [06:38<17:16, 49.38s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 60 with best_epoch = 40 and best_val_0_auc = 0.64238\n",
            " 30%|       | 9/30 [06:52<17:16, 49.38s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_auc = 0.64123\n",
            " 30%|       | 9/30 [07:07<17:16, 49.38s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_auc = 0.71841\n",
            " 30%|       | 9/30 [07:21<17:16, 49.38s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.6564\n",
            " 33%|      | 10/30 [07:32<18:59, 56.97s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 38 and best_val_0_auc = 0.67345\n",
            " 33%|      | 10/30 [07:39<18:59, 56.97s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_0_auc = 0.67748\n",
            " 33%|      | 10/30 [07:43<18:59, 56.97s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.66022\n",
            " 33%|      | 10/30 [07:48<18:59, 56.97s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.69884\n",
            " 33%|      | 10/30 [07:52<18:59, 56.97s/trial, best loss: -0.6707953708246024]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.68011\n",
            " 37%|      | 11/30 [07:57<14:52, 46.98s/trial, best loss: -0.6780188434624901]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.66551\n",
            " 37%|      | 11/30 [08:08<14:52, 46.98s/trial, best loss: -0.6780188434624901]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 44 and best_val_0_auc = 0.69301\n",
            " 37%|      | 11/30 [08:22<14:52, 46.98s/trial, best loss: -0.6780188434624901]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 67 and best_val_0_auc = 0.68905\n",
            " 37%|      | 11/30 [08:42<14:52, 46.98s/trial, best loss: -0.6780188434624901]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 71 with best_epoch = 51 and best_val_0_auc = 0.72392\n",
            " 37%|      | 11/30 [08:58<14:52, 46.98s/trial, best loss: -0.6780188434624901]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.69118\n",
            " 40%|      | 12/30 [09:06<16:06, 53.70s/trial, best loss: -0.692533635985155] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 62 and best_val_0_auc = 0.64781\n",
            " 40%|      | 12/30 [09:15<16:06, 53.70s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.66992\n",
            " 40%|      | 12/30 [09:21<16:06, 53.70s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.6837\n",
            " 40%|      | 12/30 [09:26<16:06, 53.70s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.6959\n",
            " 40%|      | 12/30 [09:32<16:06, 53.70s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 76 with best_epoch = 56 and best_val_0_auc = 0.64334\n",
            " 43%|     | 13/30 [09:41<13:35, 48.00s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.62916\n",
            " 43%|     | 13/30 [09:46<13:35, 48.00s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.67372\n",
            " 43%|     | 13/30 [09:52<13:35, 48.00s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.72304\n",
            " 43%|     | 13/30 [09:56<13:35, 48.00s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_auc = 0.69188\n",
            " 43%|     | 13/30 [10:00<13:35, 48.00s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.66484\n",
            " 47%|     | 14/30 [10:04<10:50, 40.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 93 with best_epoch = 73 and best_val_0_auc = 0.69099\n",
            " 47%|     | 14/30 [10:26<10:50, 40.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.65155\n",
            " 47%|     | 14/30 [10:36<10:50, 40.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.65921\n",
            " 47%|     | 14/30 [10:44<10:50, 40.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_0_auc = 0.68775\n",
            " 47%|     | 14/30 [11:01<10:50, 40.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.68281\n",
            " 50%|     | 15/30 [11:11<12:08, 48.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.63679\n",
            " 50%|     | 15/30 [11:23<12:08, 48.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.65899\n",
            " 50%|     | 15/30 [11:31<12:08, 48.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.6131\n",
            " 50%|     | 15/30 [11:40<12:08, 48.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 52 and best_val_0_auc = 0.66077\n",
            " 50%|     | 15/30 [11:57<12:08, 48.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.6513\n",
            " 53%|    | 16/30 [12:05<11:40, 50.03s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_auc = 0.59871\n",
            " 53%|    | 16/30 [12:12<11:40, 50.03s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 28 and best_val_0_auc = 0.68354\n",
            " 53%|    | 16/30 [12:23<11:40, 50.03s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.63883\n",
            " 53%|    | 16/30 [12:34<11:40, 50.03s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_0_auc = 0.6454\n",
            " 53%|    | 16/30 [12:40<11:40, 50.03s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 3 and best_val_0_auc = 0.61773\n",
            " 57%|    | 17/30 [12:45<10:11, 47.07s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.5594\n",
            " 57%|    | 17/30 [12:52<10:11, 47.07s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 3 and best_val_0_auc = 0.53637\n",
            " 57%|    | 17/30 [12:58<10:11, 47.07s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_0_auc = 0.5661\n",
            " 57%|    | 17/30 [13:04<10:11, 47.07s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_0_auc = 0.56174\n",
            " 57%|    | 17/30 [13:09<10:11, 47.07s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 79 with best_epoch = 59 and best_val_0_auc = 0.59798\n",
            " 60%|    | 18/30 [13:26<09:04, 45.33s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.59507\n",
            " 60%|    | 18/30 [13:34<09:04, 45.33s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 48 and best_val_0_auc = 0.65687\n",
            " 60%|    | 18/30 [13:45<09:04, 45.33s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 78 with best_epoch = 58 and best_val_0_auc = 0.6717\n",
            " 60%|    | 18/30 [13:58<09:04, 45.33s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.63702\n",
            " 60%|    | 18/30 [14:07<09:04, 45.33s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_auc = 0.66204\n",
            " 63%|   | 19/30 [14:17<08:36, 46.98s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 79 with best_epoch = 59 and best_val_0_auc = 0.6605\n",
            " 63%|   | 19/30 [14:31<08:36, 46.98s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.71312\n",
            " 63%|   | 19/30 [14:39<08:36, 46.98s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.70445\n",
            " 63%|   | 19/30 [14:48<08:36, 46.98s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.68458\n",
            " 63%|   | 19/30 [14:57<08:36, 46.98s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 69 with best_epoch = 49 and best_val_0_auc = 0.67459\n",
            " 67%|   | 20/30 [15:09<08:03, 48.36s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.63884\n",
            " 67%|   | 20/30 [15:16<08:03, 48.36s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.69337\n",
            " 67%|   | 20/30 [15:25<08:03, 48.36s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_auc = 0.6705\n",
            " 67%|   | 20/30 [15:29<08:03, 48.36s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.70538\n",
            " 67%|   | 20/30 [15:38<08:03, 48.36s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.67899\n",
            " 70%|   | 21/30 [15:47<06:47, 45.23s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.65257\n",
            " 70%|   | 21/30 [15:54<06:47, 45.23s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 52 and best_val_0_auc = 0.69375\n",
            " 70%|   | 21/30 [16:07<06:47, 45.23s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 50 and best_val_0_auc = 0.68565\n",
            " 70%|   | 21/30 [16:19<06:47, 45.23s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.675\n",
            " 70%|   | 21/30 [16:28<06:47, 45.23s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.6593\n",
            " 73%|  | 22/30 [16:37<06:14, 46.76s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.67855\n",
            " 73%|  | 22/30 [16:45<06:14, 46.76s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_auc = 0.68175\n",
            " 73%|  | 22/30 [16:54<06:14, 46.76s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_auc = 0.67859\n",
            " 73%|  | 22/30 [17:04<06:14, 46.76s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.70621\n",
            " 73%|  | 22/30 [17:15<06:14, 46.76s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.68281\n",
            " 77%|  | 23/30 [17:26<05:32, 47.53s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.65182\n",
            " 77%|  | 23/30 [17:35<05:32, 47.53s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 0.59837\n",
            " 77%|  | 23/30 [17:41<05:32, 47.53s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.5987\n",
            " 77%|  | 23/30 [17:47<05:32, 47.53s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.65291\n",
            " 77%|  | 23/30 [17:55<05:32, 47.53s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.65827\n",
            " 80%|  | 24/30 [18:04<04:27, 44.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 48 and best_val_0_auc = 0.67917\n",
            " 80%|  | 24/30 [18:16<04:27, 44.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 62 and best_val_0_auc = 0.69529\n",
            " 80%|  | 24/30 [18:30<04:27, 44.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_auc = 0.69731\n",
            " 80%|  | 24/30 [18:41<04:27, 44.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.68674\n",
            " 80%|  | 24/30 [18:49<04:27, 44.60s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 60 with best_epoch = 40 and best_val_0_auc = 0.65802\n",
            " 83%| | 25/30 [19:00<03:59, 47.99s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.65585\n",
            " 83%| | 25/30 [19:06<03:59, 47.99s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_auc = 0.67817\n",
            " 83%| | 25/30 [19:14<03:59, 47.99s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.66513\n",
            " 83%| | 25/30 [19:20<03:59, 47.99s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.71927\n",
            " 83%| | 25/30 [19:26<03:59, 47.99s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 3 and best_val_0_auc = 0.65485\n",
            " 87%| | 26/30 [19:30<02:50, 42.73s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_auc = 0.65246\n",
            " 87%| | 26/30 [19:38<02:50, 42.73s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.67573\n",
            " 87%| | 26/30 [19:47<02:50, 42.73s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 90 with best_epoch = 70 and best_val_0_auc = 0.68348\n",
            " 87%| | 26/30 [20:02<02:50, 42.73s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.71878\n",
            " 87%| | 26/30 [20:12<02:50, 42.73s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.67949\n",
            " 90%| | 27/30 [20:22<02:16, 45.47s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_0_auc = 0.63695\n",
            " 90%| | 27/30 [20:30<02:16, 45.47s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_auc = 0.65106\n",
            " 90%| | 27/30 [20:35<02:16, 45.47s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.65108\n",
            " 90%| | 27/30 [20:45<02:16, 45.47s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_auc = 0.61609\n",
            " 90%| | 27/30 [20:55<02:16, 45.47s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.64293\n",
            " 93%|| 28/30 [21:02<01:27, 43.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_auc = 0.65676\n",
            " 93%|| 28/30 [21:13<01:27, 43.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.70058\n",
            " 93%|| 28/30 [21:23<01:27, 43.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.65877\n",
            " 93%|| 28/30 [21:31<01:27, 43.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.65514\n",
            " 93%|| 28/30 [21:37<01:27, 43.63s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.66538\n",
            " 97%|| 29/30 [21:44<00:43, 43.37s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 80 with best_epoch = 60 and best_val_0_auc = 0.6893\n",
            " 97%|| 29/30 [22:02<00:43, 43.37s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.70587\n",
            " 97%|| 29/30 [22:15<00:43, 43.37s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 74 with best_epoch = 54 and best_val_0_auc = 0.71591\n",
            " 97%|| 29/30 [22:32<00:43, 43.37s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 78 with best_epoch = 58 and best_val_0_auc = 0.72028\n",
            " 97%|| 29/30 [22:49<00:43, 43.37s/trial, best loss: -0.692533635985155]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.65\n",
            "100%|| 30/30 [22:58<00:00, 45.94s/trial, best loss: -0.6962740720665268]\n",
            "[np.float64(1.0992675338862758), np.float64(1.000960344555067), np.float64(1.0269832911054881), 128, 128, 7, 42]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}